{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomics VEP Pipeline Deployment\n",
    "\n",
    "This notebook deploys the complete end-to-end genomics VEP processing pipeline.\n",
    "\n",
    "## Architecture Overview:\n",
    "1. **VCF Upload** → S3 Input Bucket triggers Lambda\n",
    "2. **VEP Workflow** → HealthOmics processes VCFs with VEP annotation\n",
    "3. **EventBridge Monitoring** → Tracks workflow completion\n",
    "4. **Variant Store Import** → Automatically imports VEP results\n",
    "5. **Lake Formation Setup** → Creates database permissions for agent queries\n",
    "\n",
    "## Prerequisites:\n",
    "- CloudFormation stack deployed using `infrastructure.yaml`\n",
    "- AWS CLI configured with appropriate permissions\n",
    "- Files in `./lambda/` directory\n",
    "\n",
    "## Deployment Steps:\n",
    "1. Configure parameters\n",
    "2. Create HealthOmics VEP workflow\n",
    "3. Create variant and annotation stores\n",
    "4. Deploy Lambda functions\n",
    "5. Configure S3 triggers\n",
    "6. Setup Lake Formation permissions\n",
    "7. Test the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import base64\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**IMPORTANT**: Update these parameters to match your CloudFormation stack and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "AWS_PROFILE = '<YOUR_AWS_PROFILE>'  # UPDATE THIS\n",
    "AWS_REGION = '<YOUR_REGION>'  # UPDATE THIS\n",
    "CLOUDFORMATION_STACK_NAME = 'genomics-vep-stack'  # UPDATE THIS\n",
    "\n",
    "# Set AWS profile\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "session = boto3.Session(profile_name=AWS_PROFILE, region_name=AWS_REGION)\n",
    "\n",
    "# Initialize clients\n",
    "cf_client = session.client('cloudformation')\n",
    "omics_client = session.client('omics')\n",
    "lambda_client = session.client('lambda')\n",
    "s3_client = session.client('s3')\n",
    "iam_client = session.client('iam')\n",
    "lakeformation = session.client('lakeformation')\n",
    "glue = session.client('glue')\n",
    "events_client = session.client('events')\n",
    "sts_client = session.client('sts')\n",
    "\n",
    "# Get account info\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(f\"Profile: {AWS_PROFILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CloudFormation Stack Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CloudFormation stack outputs\n",
    "def get_stack_outputs(stack_name):\n",
    "    try:\n",
    "        response = cf_client.describe_stacks(StackName=stack_name)\n",
    "        outputs = {}\n",
    "        for output in response['Stacks'][0].get('Outputs', []):\n",
    "            outputs[output['OutputKey']] = output['OutputValue']\n",
    "        return outputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting stack outputs: {e}\")\n",
    "        return {}\n",
    "\n",
    "stack_outputs = get_stack_outputs(CLOUDFORMATION_STACK_NAME)\n",
    "print(\"CloudFormation Stack Outputs:\")\n",
    "for key, value in stack_outputs.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Extract key values\n",
    "VCF_INPUT_BUCKET = stack_outputs.get('VcfInputBucketName')\n",
    "VEP_OUTPUT_BUCKET = stack_outputs.get('VepOutputBucketName')\n",
    "DYNAMODB_TABLE = stack_outputs.get('DynamoDBTableName')\n",
    "WORKFLOW_ROLE_ARN = stack_outputs.get('HealthOmicsWorkflowRoleArn')\n",
    "AGENT_ROLE_ARN = stack_outputs.get('AgentRoleArn')\n",
    "DATABASE_NAME = stack_outputs.get('DatabaseName')\n",
    "VCF_PROCESSOR_FUNCTION = stack_outputs.get('VcfProcessorFunctionName')\n",
    "WORKFLOW_MONITOR_FUNCTION = stack_outputs.get('WorkflowMonitorFunctionName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: HealthOmics Workflow Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for HealthOmics resources\n",
    "WORKFLOW_NAME = 'vep-workflow2'\n",
    "VARIANT_STORE_NAME = 'genomicsvariantstore'\n",
    "ANNOTATION_STORE_NAME = 'genomicsannotationstore'\n",
    "REFERENCE_STORE_ID = '<YOUR_REFERENCE_STORE_ID>'  # UPDATE your reference store ID\n",
    "REFERENCE_GENOME_ID = '8931115867' # UPDATE your reference genome ID\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "print(f\"Workflow Name: {WORKFLOW_NAME}\")\n",
    "print(f\"Variant Store: {VARIANT_STORE_NAME}\")\n",
    "print(f\"Annotation Store: {ANNOTATION_STORE_NAME}\")\n",
    "print(f\"Reference Store ID: {REFERENCE_STORE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HealthOmics VEP Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vep_workflow():\n",
    "    \"\"\"Create VEP workflow from the workflow.zip file\"\"\"\n",
    "    \n",
    "    # Read and encode the workflow.zip file\n",
    "    workflow_zip_path = './lambda/workflow.zip'\n",
    "    \n",
    "    if not os.path.exists(workflow_zip_path):\n",
    "        print(f\"Error: {workflow_zip_path} not found\")\n",
    "        return None\n",
    "    \n",
    "    with open(workflow_zip_path, 'rb') as f:\n",
    "        workflow_zip_content = f.read()\n",
    "    \n",
    "    workflow_zip_base64 = base64.b64encode(workflow_zip_content).decode('utf-8')\n",
    "    \n",
    "    # Parameter template for the VEP workflow\n",
    "    parameter_template = {\n",
    "        \"id\": {\n",
    "            \"description\": \"Sample identifier\",\n",
    "            \"optional\": False\n",
    "        },\n",
    "        \"vcf\": {\n",
    "            \"description\": \"Input VCF file path (S3 URI)\",\n",
    "            \"optional\": False\n",
    "        },\n",
    "        \"vep_species\": {\n",
    "            \"description\": \"Species name (e.g., homo_sapiens)\",\n",
    "            \"optional\": False\n",
    "        },\n",
    "        \"vep_cache\": {\n",
    "            \"description\": \"S3 path to the VEP cache directory\",\n",
    "            \"optional\": False\n",
    "        },\n",
    "        \"vep_cache_version\": {\n",
    "            \"description\": \"VEP cache version (e.g., 113)\",\n",
    "            \"optional\": False\n",
    "        },\n",
    "        \"ecr_registry\": {\n",
    "            \"description\": \"ECR registry path for container images\",\n",
    "            \"optional\": False\n",
    "        },\n",
    "        \"vep_genome\": {\n",
    "            \"description\": \"Reference genome build (e.g., GRCh38)\",\n",
    "            \"optional\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if workflow already exists\n",
    "        workflows = omics_client.list_workflows()\n",
    "        existing_workflow = None\n",
    "        \n",
    "        for workflow in workflows['items']:\n",
    "            if workflow['name'] == WORKFLOW_NAME:\n",
    "                existing_workflow = workflow\n",
    "                break\n",
    "        \n",
    "        if existing_workflow:\n",
    "            print(f\"Workflow '{WORKFLOW_NAME}' already exists with ID: {existing_workflow['id']}\")\n",
    "            return existing_workflow['id']\n",
    "        \n",
    "        # Create new workflow\n",
    "        print(f\"Creating VEP workflow: {WORKFLOW_NAME}\")\n",
    "        \n",
    "        response = omics_client.create_workflow(\n",
    "            name=WORKFLOW_NAME,\n",
    "            description='VEP (Variant Effect Predictor) workflow for genomic variant annotation',\n",
    "            engine='NEXTFLOW',\n",
    "            definitionZip=workflow_zip_content,\n",
    "            parameterTemplate=parameter_template,\n",
    "            storageCapacity=1200\n",
    "        )\n",
    "        \n",
    "        workflow_id = response['id']\n",
    "        print(f\"Workflow created successfully with ID: {workflow_id}\")\n",
    "        \n",
    "        # Wait for workflow to be ready\n",
    "        print(\"Waiting for workflow to be ready...\")\n",
    "        while True:\n",
    "            workflow_details = omics_client.get_workflow(id=workflow_id)\n",
    "            status = workflow_details['status']\n",
    "            print(f\"Workflow status: {status}\")\n",
    "            \n",
    "            if status == 'ACTIVE':\n",
    "                print(\"Workflow is ready!\")\n",
    "                break\n",
    "            elif status in ['FAILED', 'INACTIVE']:\n",
    "                print(f\"Workflow creation failed with status: {status}\")\n",
    "                if 'statusMessage' in workflow_details:\n",
    "                    print(f\"Error message: {workflow_details['statusMessage']}\")\n",
    "                return None\n",
    "            \n",
    "            time.sleep(30)\n",
    "        \n",
    "        return workflow_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating workflow: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    WORKFLOW_ID = create_vep_workflow()\n",
    "    print(f\"\\nVEP Workflow ID: {WORKFLOW_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variant Store & Annotation Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variant_store():\n",
    "    \"\"\"Create HealthOmics variant store - simple creation only\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if variant store already exists\n",
    "        variant_stores = omics_client.list_variant_stores()\n",
    "        for store in variant_stores['variantStores']:\n",
    "            if store['name'] == VARIANT_STORE_NAME:\n",
    "                print(f\"✅ Variant store '{VARIANT_STORE_NAME}' already exists with ID: {store['id']}\")\n",
    "                return store['id']\n",
    "        \n",
    "        # Create new variant store\n",
    "        print(f\"🚀 Creating variant store: {VARIANT_STORE_NAME}\")\n",
    "        \n",
    "        response = omics_client.create_variant_store(\n",
    "            name=VARIANT_STORE_NAME,\n",
    "            description='Variant store for genomics VEP pipeline',\n",
    "            reference={\n",
    "                'referenceArn': f'arn:aws:omics:{AWS_REGION}:{account_id}:referenceStore/{REFERENCE_STORE_ID}/reference/{REFERENCE_GENOME_ID}'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        variant_store_id = response['id']\n",
    "        print(f\"✅ Variant store creation initiated with ID: {variant_store_id}\")\n",
    "        print(\"⏳ Store is being created in the background...\")\n",
    "        return variant_store_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating variant store: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_annotation_store():\n",
    "    \"\"\"Create HealthOmics annotation store - simple creation only\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if annotation store already exists\n",
    "        annotation_stores = omics_client.list_annotation_stores()\n",
    "        for store in annotation_stores['annotationStores']:\n",
    "            if store['name'] == ANNOTATION_STORE_NAME:\n",
    "                print(f\"✅ Annotation store '{ANNOTATION_STORE_NAME}' already exists with ID: {store['id']}\")\n",
    "                return store['id']\n",
    "        \n",
    "        # Create new annotation store\n",
    "        print(f\"🚀 Creating annotation store: {ANNOTATION_STORE_NAME}\")\n",
    "        \n",
    "        response = omics_client.create_annotation_store(\n",
    "            name=ANNOTATION_STORE_NAME,\n",
    "            description='Annotation store for genomics VEP pipeline',\n",
    "            storeFormat='VCF',\n",
    "            reference={\n",
    "                'referenceArn': f'arn:aws:omics:{AWS_REGION}:{account_id}:referenceStore/{REFERENCE_STORE_ID}/reference/{REFERENCE_GENOME_ID}'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        annotation_store_id = response['id']\n",
    "        print(f\"✅ Annotation store creation initiated with ID: {annotation_store_id}\")\n",
    "        print(\"⏳ Store is being created in the background...\")\n",
    "        return annotation_store_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating annotation store: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the stores\n",
    "VARIANT_STORE_ID = create_variant_store()\n",
    "ANNOTATION_STORE_ID = create_annotation_store()\n",
    "\n",
    "print(f\"\\n📋 Summary:\")\n",
    "print(f\"Variant Store ID: {VARIANT_STORE_ID}\")\n",
    "print(f\"Annotation Store ID: {ANNOTATION_STORE_ID}\")\n",
    "print(f\"\\n💡 Use the status checker below to monitor progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_status_check(store_name_or_id):\n",
    "    try:\n",
    "        store = omics_client.get_variant_store(name=store_name_or_id)\n",
    "        return f\"Status: {store['status']} | ID: {store['id']}\"\n",
    "    except:\n",
    "        return \"Store not found or still creating\"\n",
    "\n",
    "# Usage\n",
    "print(quick_status_check('genomicsvariantstore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_status_check(store_name_or_id):\n",
    "    try:\n",
    "        store = omics_client.get_annotation_store(name=store_name_or_id)\n",
    "        return f\"Status: {store['status']} | ID: {store['id']}\"\n",
    "    except:\n",
    "        return \"Store not found or still creating\"\n",
    "\n",
    "# Usage\n",
    "print(quick_status_check('genomicsannotationstore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Lambda Function Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy VCF Processor Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_vcf_processor_lambda():\n",
    "    \"\"\"Deploy the VCF processor Lambda function with proper update handling\"\"\"\n",
    "    \n",
    "    lambda_zip_path = './lambda/genomics-vep-pipeline-vcf-processor-comprehensive.zip'\n",
    "    \n",
    "    if not os.path.exists(lambda_zip_path):\n",
    "        print(f\"Error: {lambda_zip_path} not found\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Read the Lambda function code\n",
    "        with open(lambda_zip_path, 'rb') as f:\n",
    "            lambda_zip_content = f.read()\n",
    "        \n",
    "        # Step 1: Update the Lambda function code\n",
    "        print(f\"🔄 Updating Lambda function code: {VCF_PROCESSOR_FUNCTION}\")\n",
    "        \n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=VCF_PROCESSOR_FUNCTION,\n",
    "            ZipFile=lambda_zip_content\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Lambda function code updated successfully\")\n",
    "        \n",
    "        # Step 2: Wait for the code update to complete before updating configuration\n",
    "        print(\"⏳ Waiting for code update to complete...\")\n",
    "        \n",
    "        max_attempts = 30  # 5 minutes max\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                function_info = lambda_client.get_function(FunctionName=VCF_PROCESSOR_FUNCTION)\n",
    "                last_update_status = function_info['Configuration']['LastUpdateStatus']\n",
    "                \n",
    "                print(f\"   Attempt {attempt + 1}: Status = {last_update_status}\")\n",
    "                \n",
    "                if last_update_status == 'Successful':\n",
    "                    print(\"✅ Code update completed successfully\")\n",
    "                    break\n",
    "                elif last_update_status == 'Failed':\n",
    "                    print(\"❌ Code update failed\")\n",
    "                    return False\n",
    "                elif last_update_status in ['InProgress']:\n",
    "                    print(\"   Still updating code...\")\n",
    "                else:\n",
    "                    print(f\"   Unknown status: {last_update_status}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error checking status: {e}\")\n",
    "            \n",
    "            time.sleep(10)  # Wait 10 seconds between checks\n",
    "        else:\n",
    "            print(\"⚠️  Timeout waiting for code update to complete\")\n",
    "            print(\"💡 Trying configuration update anyway...\")\n",
    "        \n",
    "        # Step 3: Update environment variables\n",
    "        print(\"🔧 Updating environment variables...\")\n",
    "        \n",
    "        env_vars = {\n",
    "            'WORKFLOW_ID': str(WORKFLOW_ID) if WORKFLOW_ID else '',\n",
    "            'ROLE_ARN': WORKFLOW_ROLE_ARN,\n",
    "            'OUTPUT_URI': f's3://{VEP_OUTPUT_BUCKET}',\n",
    "            'DYNAMODB_TABLE': DYNAMODB_TABLE,\n",
    "            'BATCH_SIZE': str(BATCH_SIZE),\n",
    "            'ALLOWED_PREFIXES': ''  # Empty means all prefixes allowed\n",
    "        }\n",
    "        \n",
    "        # Wait a bit more before configuration update\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Retry configuration update with exponential backoff\n",
    "        max_config_attempts = 5\n",
    "        for config_attempt in range(max_config_attempts):\n",
    "            try:\n",
    "                lambda_client.update_function_configuration(\n",
    "                    FunctionName=VCF_PROCESSOR_FUNCTION,\n",
    "                    Environment={'Variables': env_vars}\n",
    "                )\n",
    "                \n",
    "                print(\"✅ Environment variables updated successfully\")\n",
    "                break\n",
    "                \n",
    "            except lambda_client.exceptions.ResourceConflictException as e:\n",
    "                wait_time = 2 ** config_attempt  # Exponential backoff: 1, 2, 4, 8, 16 seconds\n",
    "                print(f\"   Configuration update conflict (attempt {config_attempt + 1}). Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "                if config_attempt == max_config_attempts - 1:\n",
    "                    print(\"❌ Failed to update configuration after multiple attempts\")\n",
    "                    print(\"💡 You can update environment variables manually in the AWS Console\")\n",
    "                    return False\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error updating configuration: {e}\")\n",
    "                return False\n",
    "        \n",
    "        print(\"Environment variables updated:\")\n",
    "        for key, value in env_vars.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deploying VCF processor Lambda: {e}\")\n",
    "        return False\n",
    "\n",
    "# Deploy VCF processor Lambda\n",
    "vcf_processor_deployed = deploy_vcf_processor_lambda()\n",
    "print(f\"\\n📊 VCF Processor Lambda deployed: {vcf_processor_deployed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Workflow Monitor Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_workflow_monitor_lambda():\n",
    "    \"\"\"Deploy the workflow monitor Lambda function with proper update handling\"\"\"\n",
    "    \n",
    "    lambda_zip_path = './lambda/genomics-vep-pipeline-workflow-monitor-fixed.zip'\n",
    "    \n",
    "    if not os.path.exists(lambda_zip_path):\n",
    "        print(f\"Error: {lambda_zip_path} not found\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Read the Lambda function code\n",
    "        with open(lambda_zip_path, 'rb') as f:\n",
    "            lambda_zip_content = f.read()\n",
    "        \n",
    "        # Step 1: Update the Lambda function code\n",
    "        print(f\"🔄 Updating Lambda function code: {WORKFLOW_MONITOR_FUNCTION}\")\n",
    "        \n",
    "        response = lambda_client.update_function_code(\n",
    "            FunctionName=WORKFLOW_MONITOR_FUNCTION,\n",
    "            ZipFile=lambda_zip_content\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Lambda function code updated successfully\")\n",
    "        \n",
    "        # Step 2: Wait for the code update to complete before updating configuration\n",
    "        print(\"⏳ Waiting for code update to complete...\")\n",
    "        \n",
    "        max_attempts = 30  # 5 minutes max\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                function_info = lambda_client.get_function(FunctionName=WORKFLOW_MONITOR_FUNCTION)\n",
    "                last_update_status = function_info['Configuration']['LastUpdateStatus']\n",
    "                \n",
    "                print(f\"   Attempt {attempt + 1}: Status = {last_update_status}\")\n",
    "                \n",
    "                if last_update_status == 'Successful':\n",
    "                    print(\"✅ Code update completed successfully\")\n",
    "                    break\n",
    "                elif last_update_status == 'Failed':\n",
    "                    print(\"❌ Code update failed\")\n",
    "                    return False\n",
    "                elif last_update_status in ['InProgress']:\n",
    "                    print(\"   Still updating code...\")\n",
    "                else:\n",
    "                    print(f\"   Unknown status: {last_update_status}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error checking status: {e}\")\n",
    "            \n",
    "            time.sleep(10)  # Wait 10 seconds between checks\n",
    "        else:\n",
    "            print(\"⚠️  Timeout waiting for code update to complete\")\n",
    "            print(\"💡 Trying configuration update anyway...\")\n",
    "        \n",
    "        # Step 3: Update environment variables\n",
    "        print(\"🔧 Updating environment variables...\")\n",
    "        \n",
    "        env_vars = {\n",
    "            'ROLE_ARN': WORKFLOW_ROLE_ARN,\n",
    "            'VARIANT_STORE_NAME': VARIANT_STORE_NAME,\n",
    "            'ANNOTATION_STORE_NAME': ANNOTATION_STORE_NAME,\n",
    "            'DYNAMODB_TABLE': DYNAMODB_TABLE,\n",
    "            'DATABASE_NAME': DATABASE_NAME\n",
    "        }\n",
    "        \n",
    "        # Wait a bit more before configuration update\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Retry configuration update with exponential backoff\n",
    "        max_config_attempts = 5\n",
    "        for config_attempt in range(max_config_attempts):\n",
    "            try:\n",
    "                lambda_client.update_function_configuration(\n",
    "                    FunctionName=WORKFLOW_MONITOR_FUNCTION,\n",
    "                    Environment={'Variables': env_vars}\n",
    "                )\n",
    "                \n",
    "                print(\"✅ Environment variables updated successfully\")\n",
    "                break\n",
    "                \n",
    "            except lambda_client.exceptions.ResourceConflictException as e:\n",
    "                wait_time = 2 ** config_attempt  # Exponential backoff: 1, 2, 4, 8, 16 seconds\n",
    "                print(f\"   Configuration update conflict (attempt {config_attempt + 1}). Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "                if config_attempt == max_config_attempts - 1:\n",
    "                    print(\"❌ Failed to update configuration after multiple attempts\")\n",
    "                    print(\"💡 You can update environment variables manually in the AWS Console\")\n",
    "                    return False\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error updating configuration: {e}\")\n",
    "                return False\n",
    "        \n",
    "        print(\"Environment variables updated:\")\n",
    "        for key, value in env_vars.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error deploying workflow monitor Lambda: {e}\")\n",
    "        return False\n",
    "\n",
    "# Deploy workflow monitor Lambda\n",
    "workflow_monitor_deployed = deploy_workflow_monitor_lambda()\n",
    "print(f\"\\n📊 Workflow Monitor Lambda deployed: {workflow_monitor_deployed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure S3 Event Notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_s3_notifications_fixed():\n",
    "    \"\"\"Configure S3 event notifications with correct parameter names\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 🔹 INPUT BUCKET CONFIGURATION\n",
    "        print(f\"Configuring S3 notifications for input bucket: {VCF_INPUT_BUCKET}\")\n",
    "        \n",
    "        vcf_input_notification = {\n",
    "            'LambdaFunctionConfigurations': [  # ✅ Fixed: was 'LambdaConfigurations'\n",
    "                {\n",
    "                    'Id': 'VcfProcessorTrigger',\n",
    "                    'LambdaFunctionArn': f'arn:aws:lambda:{AWS_REGION}:{account_id}:function:{VCF_PROCESSOR_FUNCTION}',\n",
    "                    'Events': ['s3:ObjectCreated:*'],\n",
    "                    'Filter': {\n",
    "                        'Key': {\n",
    "                            'FilterRules': [\n",
    "                                {'Name': 'suffix', 'Value': '.vcf'}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'Id': 'VcfGzProcessorTrigger',\n",
    "                    'LambdaFunctionArn': f'arn:aws:lambda:{AWS_REGION}:{account_id}:function:{VCF_PROCESSOR_FUNCTION}',\n",
    "                    'Events': ['s3:ObjectCreated:*'],\n",
    "                    'Filter': {\n",
    "                        'Key': {\n",
    "                            'FilterRules': [\n",
    "                                {'Name': 'suffix', 'Value': '.vcf.gz'}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        s3_client.put_bucket_notification_configuration(\n",
    "            Bucket=VCF_INPUT_BUCKET,\n",
    "            NotificationConfiguration=vcf_input_notification\n",
    "        )\n",
    "        \n",
    "        print(\"✅ VCF input bucket notifications configured successfully\")\n",
    "        \n",
    "        # 🔹 OUTPUT BUCKET CONFIGURATION\n",
    "        print(f\"Configuring S3 notifications for output bucket: {VEP_OUTPUT_BUCKET}\")\n",
    "        \n",
    "        vep_output_notification = {\n",
    "            'LambdaFunctionConfigurations': [  # ✅ Fixed: was 'LambdaConfigurations'\n",
    "                {\n",
    "                    'Id': 'VepAnnotatedVcfTrigger',\n",
    "                    'LambdaFunctionArn': f'arn:aws:lambda:{AWS_REGION}:{account_id}:function:{WORKFLOW_MONITOR_FUNCTION}',\n",
    "                    'Events': ['s3:ObjectCreated:*'],\n",
    "                    'Filter': {\n",
    "                        'Key': {\n",
    "                            'FilterRules': [\n",
    "                                {'Name': 'suffix', 'Value': '.ann.vcf.gz'}  # VEP annotated files\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'Id': 'VepAnnotatedVcfUncompressedTrigger',\n",
    "                    'LambdaFunctionArn': f'arn:aws:lambda:{AWS_REGION}:{account_id}:function:{WORKFLOW_MONITOR_FUNCTION}',\n",
    "                    'Events': ['s3:ObjectCreated:*'],\n",
    "                    'Filter': {\n",
    "                        'Key': {\n",
    "                            'FilterRules': [\n",
    "                                {'Name': 'suffix', 'Value': '.ann.vcf'}  # VEP annotated files (uncompressed)\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        s3_client.put_bucket_notification_configuration(\n",
    "            Bucket=VEP_OUTPUT_BUCKET,\n",
    "            NotificationConfiguration=vep_output_notification\n",
    "        )\n",
    "        \n",
    "        print(\"✅ VEP output bucket notifications configured successfully\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error configuring S3 notifications: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure EventBridge monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_eventbridge_workflow_monitoring():\n",
    "    \"\"\"Configure EventBridge to monitor HealthOmics workflow completion\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check if EventBridge rule already exists\n",
    "        existing_rules = events_client.list_rules(NamePrefix=f'{CLOUDFORMATION_STACK_NAME}-workflow-status')\n",
    "        \n",
    "        if existing_rules['Rules']:\n",
    "            print(\"✅ EventBridge workflow monitoring rule already exists\")\n",
    "            return True\n",
    "        \n",
    "        # Create EventBridge rule for workflow completion\n",
    "        rule_name = f'{CLOUDFORMATION_STACK_NAME}-workflow-completion-monitor'\n",
    "        \n",
    "        print(f\"🔧 Creating EventBridge rule: {rule_name}\")\n",
    "        \n",
    "        # Create the rule\n",
    "        events_client.put_rule(\n",
    "            Name=rule_name,\n",
    "            EventPattern=json.dumps({\n",
    "                \"source\": [\"aws.omics\"],\n",
    "                \"detail-type\": [\"HealthOmics Run Status Change\"],\n",
    "                \"detail\": {\n",
    "                    \"status\": [\"COMPLETED\", \"FAILED\", \"CANCELLED\"],\n",
    "                    \"workflowId\": [str(WORKFLOW_ID)] if WORKFLOW_ID else []\n",
    "                }\n",
    "            }),\n",
    "            State='ENABLED',\n",
    "            Description='Monitor VEP workflow completion for automatic variant import'\n",
    "        )\n",
    "        \n",
    "        # Add Lambda target\n",
    "        events_client.put_targets(\n",
    "            Rule=rule_name,\n",
    "            Targets=[\n",
    "                {\n",
    "                    'Id': '1',\n",
    "                    'Arn': f'arn:aws:lambda:{AWS_REGION}:{account_id}:function:{WORKFLOW_MONITOR_FUNCTION}',\n",
    "                    'InputTransformer': {\n",
    "                        'InputPathsMap': {\n",
    "                            'runId': '$.detail.runId',\n",
    "                            'status': '$.detail.status',\n",
    "                            'workflowId': '$.detail.workflowId'\n",
    "                        },\n",
    "                        'InputTemplate': '{\"source\": \"eventbridge\", \"runId\": \"<runId>\", \"status\": \"<status>\", \"workflowId\": \"<workflowId>\"}'\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"✅ EventBridge workflow monitoring configured successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error configuring EventBridge: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_complete_pipeline_triggers():\n",
    "    \"\"\"Configure both S3 and EventBridge triggers for complete pipeline\"\"\"\n",
    "    \n",
    "    print(\"🚀 Configuring Complete Pipeline Triggers\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Configure S3 notifications for VCF input (triggers VEP workflow)\n",
    "    print(\"1️⃣ Configuring S3 notifications for VCF input...\")\n",
    "    s3_success = configure_s3_notifications_fixed()\n",
    "    \n",
    "    # 2. Configure EventBridge for workflow completion (triggers variant import)\n",
    "    print(\"\\n2️⃣ Configuring EventBridge for workflow monitoring...\")\n",
    "    eventbridge_success = configure_eventbridge_workflow_monitoring()\n",
    "    \n",
    "    # 3. Optional: Configure S3 notifications for VEP output (backup trigger)\n",
    "    print(\"\\n3️⃣ S3 notifications for VEP output configured as backup\")\n",
    "    \n",
    "    print(f\"\\n📊 Configuration Summary:\")\n",
    "    print(f\"   S3 Input Triggers: {'✅ Success' if s3_success else '❌ Failed'}\")\n",
    "    print(f\"   EventBridge Monitoring: {'✅ Success' if eventbridge_success else '❌ Failed'}\")\n",
    "    \n",
    "    print(f\"\\n🔄 Pipeline Flow:\")\n",
    "    print(f\"   1. Upload VCF → S3 Input Bucket → VCF Processor Lambda → VEP Workflow\")\n",
    "    print(f\"   2. VEP Workflow Complete → EventBridge → Workflow Monitor Lambda → Variant Import\")\n",
    "    print(f\"   3. Variant Import Complete → Data ready for Agent queries\")\n",
    "    \n",
    "    return s3_success and eventbridge_success\n",
    "\n",
    "# Configure the complete pipeline\n",
    "pipeline_configured = configure_complete_pipeline_triggers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3b: Running analytics on healthomics variant store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a service IAM role\n",
    "\n",
    "For the purposes of this tutorial, we will use the following policy and trust policy to demo the capabilities of AWS HealthOmics, you are free to customize permissions as required for your use case after going though this tutorial.\n",
    "\n",
    "NOTE: In this case we've defined rather permissive permissions (i.e. \"*\" Resources). In particular, we are allowing read/write access to all S3 buckets available to the account for this tutorial. In a real world setting you will want to scope this down to only the minimally needed actions on necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a timestamp\n",
    "dt_fmt = '%Y%m%dT%H%M%S'\n",
    "ts = datetime.now().strftime(dt_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"omics:*\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"ram:AcceptResourceShareInvitation\",\n",
    "        \"ram:GetResourceShareInvitations\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"s3:GetBucketLocation\",\n",
    "        \"s3:PutObject\",\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:ListBucket\",\n",
    "        \"s3:AbortMultipartUpload\",\n",
    "        \"s3:ListMultipartUploadParts\",\n",
    "        \"s3:GetObjectAcl\",\n",
    "        \"s3:PutObjectAcl\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "demo_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"omics.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to proceed we need to create a couple of resources the first is the role that you will be passing into AWS HealthOmics. If the role doesn't exist, we will need to create it and attach the policy and trust policy defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this as the base name for our role and policy\n",
    "omics_iam_name = f'OmicsTutorialRole-{ts}'\n",
    "\n",
    "# Create the iam client\n",
    "iam = boto3.resource('iam')\n",
    "\n",
    "# Check if the role already exist if not create it\n",
    "try:\n",
    "    role = iam.Role(omics_iam_name)\n",
    "    role.load()\n",
    "    \n",
    "except botocore.exceptions.ClientError as ex:\n",
    "    if ex.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n",
    "        #Create the role with the corresponding trust policy\n",
    "        role = iam.create_role(\n",
    "            RoleName=omics_iam_name, \n",
    "            AssumeRolePolicyDocument=json.dumps(demo_trust_policy))\n",
    "        \n",
    "        #Create policy\n",
    "        policy = iam.create_policy(\n",
    "            PolicyName='{}-policy'.format(omics_iam_name), \n",
    "            Description=\"Policy for AWS HealthOmics demo\",\n",
    "            PolicyDocument=json.dumps(demo_policy))\n",
    "        \n",
    "        #Attach the policy to the role\n",
    "        policy.attach_role(RoleName=omics_iam_name)\n",
    "    else:\n",
    "        print('Something went wrong, please retry and check your account settings and permissions')\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the role exists, lets create a helper function to help us retrieve the role arn which we will need to pass into the service API calls. The role arn will grant AWS HealthOmics the permissions it needs to access the resources it needs in your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics_iam_name = 'OmicsTutorialRole-20250818T091459'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_arn(role_name):\n",
    "    try:\n",
    "        iam = boto3.resource('iam')\n",
    "        role = iam.Role(role_name)\n",
    "        role.load()  # calls GetRole to load attributes\n",
    "    except ClientError:\n",
    "        print(\"Couldn't get role named %s.\"%role_name)\n",
    "        raise\n",
    "    else:\n",
    "        return role.arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the AWS HealthOmics client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "omics = boto3.client('omics')\n",
    "\n",
    "region = omics.meta.region_name\n",
    "regional_bucket = f'<YOUR_SOURCE_BUCKET>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the created variant store and vcf files imported into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_store = omics.get_variant_store(name='genomicsvariantstore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics.list_variant_import_jobs(filter={\"storeName\": var_store['name']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Query in Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to query your data in Amazon Athena, you need to create resource links to your database using the AWS Lake Formation Console. You will also need to ensure that the IAM user running this notebook is a Data Lake Administrator. Note without both of these in place, the following queries will fail. To satisfy these prerequisites, refer to the instructions provided in the AWS Lake Formation documentation and AWS HealthOmics documentation.\n",
    "\n",
    "The following code will create resource links to the default database in the AwsDataCatalog in AWS Glue. It makes a few assumptions to do so - like IAM identity you are using to run this notebook is a Data Lake Administrator and has the permissions to create AWS Glue tables.\n",
    "\n",
    "If you want to be fully sure you are making the correct resource links and providing access to them to the correct identities it is best to create them directly. Refer to the instructions in the AWS HealthOmics documentation on how to do this.\n",
    "\n",
    "We'll need to work with AWS RAM, AWS Glue, and AWS Lake Formation to setup resource links and grant database permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram = boto3.client('ram')\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "caller_identity = boto3.client('sts').get_caller_identity()\n",
    "AWS_ACCOUNT_ID = caller_identity['Account']\n",
    "AWS_IDENITY_ARN = caller_identity['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll list available shared resources from OTHER-ACCOUNTS in AWS RAM and look for the resource that matches the id of the Variant store we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = ram.list_resources(resourceOwner='OTHER-ACCOUNTS', resourceType='glue:Database')\n",
    "\n",
    "if not response.get('resources'):\n",
    "    print('no shared resources found. verify that you have successfully created an HealthOmics Analytics store')\n",
    "else:\n",
    "    variantstore_resources = [resource for resource in response['resources'] if var_store['id'] in resource['arn']]\n",
    "    if not variantstore_resources:\n",
    "        print(f\"no shared resources matching variant store id {var_store['id']} found\")\n",
    "    else:\n",
    "        variantstore_resource = variantstore_resources[0]\n",
    "\n",
    "variantstore_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get the specific resource share the Variant store is associated with. This is so we can get the owningAccountId attribute for the share. (Note we could also do this by parsing the resourceShareArn for the resource above, but doing it this way is more explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_share = ram.get_resource_shares(\n",
    "    resourceOwner='OTHER-ACCOUNTS', \n",
    "    resourceShareArns=[variantstore_resource['resourceShareArn']])['resourceShares'][0]\n",
    "resource_share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a table in AWS Glue for the variant store. This is the same as creating a resource link in AWS Lake Formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this creates a resource link to the table for the variant store and adds it to the `default` database\n",
    "glue.create_table(\n",
    "    DatabaseName='<YOUR_AWS_PROFILE>',\n",
    "    TableInput = {\n",
    "        \"Name\": var_store['name'],\n",
    "        \"TargetTable\": {\n",
    "            \"CatalogId\": resource_share['owningAccountId'],\n",
    "            \"DatabaseName\": f\"variant_{AWS_ACCOUNT_ID}_{var_store['id']}\",\n",
    "            \"Name\": var_store['name'],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section of the tutorial, the identity that runs this notebook either:\n",
    "\n",
    "1. needs to be a Data lake administrator in AWS Lake Formation, or\n",
    "2. must be granted access to the AWS RAM shared resources by an existing administrator.\n",
    "The latter pattern is recommended. Both DESCRIBE and SELECT on the target table for the variant store are required and can be done via the \"Grant on target\" action on a resource link in the AWS Lake Formation Console. You can also do this with and admin identity via the SDK with code like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = boto3.client('sts')\n",
    "identity = sts.get_caller_identity()\n",
    "\n",
    "# Extract the role ARN from the assumed role ARN\n",
    "assumed_role_arn = identity['Arn']\n",
    "# Convert from: <YOUR_ASSUMED_ROLE_ARN>\n",
    "# To: arn:aws:iam::<YOUR_ACCOUNT_ID>:role/SageMakerNotebookInstanceRole\n",
    "role_arn = assumed_role_arn.replace(':sts:', ':iam:').replace(':assumed-role/', ':role/').rsplit('/', 1)[0]\n",
    "\n",
    "lfn = boto3.client('lakeformation')\n",
    "\n",
    "lfn.grant_permissions(\n",
    "    Principal={\n",
    "        \"DataLakePrincipalIdentifier\": role_arn\n",
    "    },\n",
    "    Resource={\n",
    "        \"Table\": {\n",
    "            \"CatalogId\": resource_share['owningAccountId'],\n",
    "            \"DatabaseName\": f\"variant_{AWS_ACCOUNT_ID}_{var_store['id']}\",\n",
    "            \"Name\": var_store['name']\n",
    "        }\n",
    "    },\n",
    "    Permissions=[ 'DESCRIBE' ],\n",
    "    PermissionsWithGrantOption=[ 'DESCRIBE' ]\n",
    ")\n",
    "\n",
    "lfn.grant_permissions(\n",
    "    Principal={\n",
    "        \"DataLakePrincipalIdentifier\": role_arn\n",
    "    },\n",
    "    Resource={\n",
    "        \"TableWithColumns\": {\n",
    "            \"CatalogId\": resource_share['owningAccountId'],\n",
    "            \"DatabaseName\": f\"variant_{AWS_ACCOUNT_ID}_{var_store['id']}\",\n",
    "            \"Name\": var_store['name'],\n",
    "            \"ColumnWildcard\": {}\n",
    "        }\n",
    "    },\n",
    "    Permissions=[ 'SELECT' ],\n",
    "    PermissionsWithGrantOption=[ 'SELECT' ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have resource links created, we can start querying the data using Amazon Athena. You don't need to wait for all the import jobs to complete to start doing this. Queries can be made while data imports in the background.\n",
    "\n",
    "To query AWS HealthOmics Analytics stores, you need to use Athena engine version 3. The following code checks if you have an existing Athena workgroup that satisfies this criteria. If not it will create one called omics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client('athena')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_workgroups = athena.list_work_groups()['WorkGroups']\n",
    "athena_workgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_workgroups = athena.list_work_groups()['WorkGroups']\n",
    "\n",
    "athena_workgroup = None\n",
    "for wg in athena_workgroups:\n",
    "    print(wg['EngineVersion']['EffectiveEngineVersion'])\n",
    "    if wg['EngineVersion']['EffectiveEngineVersion'] == 'Athena engine version 3':\n",
    "        print(f\"Workgroup '{wg['Name']}' found using Athena engine version 3\")\n",
    "        athena_workgroup = wg\n",
    "        break\n",
    "else:\n",
    "    print(\"No workgroups with Athena engine version 3 found. creating one\")\n",
    "    athena_workgroup = athena.create_work_group(\n",
    "        Name='omics',\n",
    "        Configuration={\n",
    "            \"EngineVersion\": {\n",
    "                \"SelectedEngineVersion\": \"Athena engine version 3\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "athena_workgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start writing queries!\n",
    "\n",
    "For fun, let's calculate the TI/TV ratio for these samples. You can navigate to the Athena console or do this from your Jupyter Notebook. This example uses the workgroup omics and assumes you have made a resource link to your Variant store in your default database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_query = f\"\"\"SELECT * FROM \"default\".\"{var_store['name']}\" LIMIT 10\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "df_titv = wr.athena.read_sql_query(\n",
    "    simple_query, \n",
    "    database='<YOUR_AWS_PROFILE>', \n",
    "    workgroup=athena_workgroup['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_query = f\"\"\"SELECT DISTINCT sampleid FROM \"default\".\"{var_store['name']}\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = wr.athena.read_sql_query(\n",
    "    count_query, \n",
    "    database='<YOUR_AWS_PROFILE>', \n",
    "    workgroup=athena_workgroup['Name'])\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://<YOUR_SOURCE_BUCKET>/dragen_vcfs2/NA21135.hard-filtered.vcf.gz s3://<YOUR_VCF_INPUT_BUCKET>/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws dynamodb scan --table-name genomics-vep-pipeline-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://<YOUR_SOURCE_BUCKET>/dragen_vcfs2/NA21144.hard-filtered.vcf.gz s3://<YOUR_VCF_INPUT_BUCKET>/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up an Annotation store.\n",
    "\n",
    "AWS HealthOmics Annotation stores support annotations in VCF, GFF, and TSV formats. In this tutorial, we import ClinVar annotations which can be downloaded from the NCBI as a VCF file. Imports need to come from an S3 location in the same region, so we'll use a copy in a regional bucket for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar_20250810.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp clinvar_20250810.vcf.gz s3://<YOUR_SOURCE_BUCKET>/clinvar20250810/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_ANNOTATION_URI = f\"s3://<YOUR_SOURCE_BUCKET>/clinvar20250810/clinvar_20250810.vcf.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, importing data into, and querying an Annotation store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of creating, importing data into, and querying an Annotation store is similar to the process you did above for the Variant store, so we'll be brief on the descriptions of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_store_name = f'genomicsannotationstore'\n",
    "ref_name = 'dragen-ref-genome3'  ## Change this reference name to match one you have created if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = omics.start_annotation_import_job(\n",
    "    destinationName=f'genomicsannotationstore',\n",
    "    roleArn=get_role_arn(omics_iam_name),\n",
    "    items=[{\"source\": SOURCE_ANNOTATION_URI}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics.list_annotation_import_jobs(filter={\"storeName\": ann_store_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_store = omics.get_annotation_store(name='genomicsannotationstore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a resource link to the Annotation store is the same as with the Variant store. We'll do this all in one cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ram.list_resources(resourceOwner='OTHER-ACCOUNTS', resourceType='glue:Database')\n",
    "\n",
    "if not response.get('resources'):\n",
    "    print('no shared resources found. verify that you have successfully created an HealthOmics Analytics store')\n",
    "else:\n",
    "    annotationstore_resources = [resource for resource in response['resources'] if ann_store['id'] in resource['arn']]\n",
    "    if not annotationstore_resources:\n",
    "        print(f\"no shared resources matching annotation store id {ann_store['id']} found\")\n",
    "    else:\n",
    "        annotationstore_resource = annotationstore_resources[0]\n",
    "\n",
    "        resource_share = ram.get_resource_shares(\n",
    "            resourceOwner='OTHER-ACCOUNTS', \n",
    "            resourceShareArns=[annotationstore_resource['resourceShareArn']])['resourceShares'][0]\n",
    "        \n",
    "        # this creates a resource link to the table for the annotation store and adds it to the `default` database\n",
    "        glue.create_table(\n",
    "            DatabaseName='<YOUR_AWS_PROFILE>',\n",
    "            TableInput = {\n",
    "                \"Name\": ann_store['name'],\n",
    "                \"TargetTable\": {\n",
    "                    \"CatalogId\": resource_share['owningAccountId'],\n",
    "                    \"DatabaseName\": f\"annotation_{AWS_ACCOUNT_ID}_{ann_store['id']}\",\n",
    "                    \"Name\": ann_store['name'],\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_query = f\"\"\"SELECT * FROM \"default\".\"{ann_store['name']}\" LIMIT 10\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annot = wr.athena.read_sql_query(\n",
    "    annot_query, \n",
    "    database='<YOUR_AWS_PROFILE>', \n",
    "    workgroup=athena_workgroup['Name'])\n",
    "df_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_query = f\"\"\"SELECT \n",
    "    v.sampleid,\n",
    "    v.contigname,\n",
    "    v.start,\n",
    "    v.referenceallele,\n",
    "    v.alternatealleles,\n",
    "    a.names as clinvar_variation_id,\n",
    "    -- Extract clinical significance from attributes map\n",
    "    CASE \n",
    "        WHEN a.attributes['CLNSIG'] = 'Pathogenic' THEN 'Pathogenic'\n",
    "        WHEN a.attributes['CLNSIG'] = 'Likely_pathogenic' THEN 'Likely Pathogenic'\n",
    "        WHEN a.attributes['CLNSIG'] = 'Uncertain_significance' THEN 'Uncertain Significance'\n",
    "        WHEN a.attributes['CLNSIG'] = 'Likely_benign' THEN 'Likely Benign'\n",
    "        WHEN a.attributes['CLNSIG'] = 'Benign' THEN 'Benign'\n",
    "        ELSE 'Unknown'\n",
    "    END as clinical_significance,\n",
    "    -- Extract gene name from GENEINFO (split on colon and take first part)\n",
    "    CASE \n",
    "        WHEN a.attributes['GENEINFO'] IS NOT NULL \n",
    "        THEN split_part(a.attributes['GENEINFO'], ':', 1)\n",
    "        ELSE NULL\n",
    "    END as gene_name,\n",
    "    -- Keep full gene info for reference\n",
    "    a.attributes['GENEINFO'] as full_gene_info\n",
    "FROM \"default\".\"{var_store['name']}\" v\n",
    "INNER JOIN \"default\".\"{ann_store['name']}\" a ON (\n",
    "    REPLACE(v.contigname, 'chr', '') = a.contigname\n",
    "    AND v.start = a.start\n",
    "    AND v.referenceallele = a.referenceallele\n",
    "    AND v.alternatealleles = a.alternatealleles\n",
    ")\n",
    "WHERE a.attributes IS NOT NULL \n",
    "    AND a.attributes['CLNSIG'] IS NOT NULL\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_explore_annot = wr.athena.read_sql_query(\n",
    "    comprehensive_query_limited, \n",
    "    database='<YOUR_AWS_PROFILE>', \n",
    "    workgroup=athena_workgroup['Name'])\n",
    "v_explore_annot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

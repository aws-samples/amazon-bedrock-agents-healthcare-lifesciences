AWSTemplateFormatVersion: "2010-09-09"
Description: Creates infrastructure dependencies for cancer biomarker discovery workshop

Parameters:
  BedrockModelId:
    Type: String
    Description: The ID of the Foundation Model to use for the Agent
    Default: us.anthropic.claude-3-5-sonnet-20241022-v2:0
  EnvironmentName:
    Type: String
    Description: The name of the agent environment, used to differentiate agent application. Must be lowercase, contain one number, and be no more than 5 characters long.
    Default: env1
    MaxLength: 5
    AllowedPattern: ^[a-z]{1,4}[0-9]$
    ConstraintDescription: Must be lowercase, contain one number at the end, and be no more than 5 characters long.
  RedshiftDatabaseName:
    Type: String
    Default: dev
  RedshiftUserName:
    Type: String
    Default: admin
  RedshiftPassword:
    Type: String
    NoEcho: true
    Default: 'MyPassword123'  
    Description: 'The password for the Redshift master user. Must be at least 8 characters long and contain at least one uppercase letter, one lowercase letter, and one number.'
    MinLength: 8
    MaxLength: 64
    AllowedPattern: ^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)[a-zA-Z\d!@#$%^&*()_+\-=\[\]{};:'",.<>?]{8,64}$
    ConstraintDescription: 'Password must be between 8 and 64 characters, and contain at least one uppercase letter, one lowercase letter, and one number.'
  GithubLink:
   Type: String
   Description: 'The link to the agent build cloudformation stack'
   Default: 'https://github.com/aws-samples/amazon-bedrock-agents-cancer-biomarker-discovery.git'
  ImageTag:
    Type: String
    Default: latest
    Description: Tag of the Docker image to deploy Streamlit UI
  VectorStoreName:
    Type: String
    Description: The name of the vectorstore
    Default: nmcicollection
  IndexName:
    Type: String
    Description: The name of the vector index
    Default: vector-index

Mappings:
  RegionMap:
    us-east-1:
      PandasLayer: 'arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python39:20'
    us-east-2:
      PandasLayer: 'arn:aws:lambda:us-east-2:336392948345:layer:AWSSDKPandas-Python39:20'
    us-west-1:
      PandasLayer: 'arn:aws:lambda:us-west-1:336392948345:layer:AWSSDKPandas-Python39:20'
    us-west-2:
      PandasLayer: 'arn:aws:lambda:us-west-2:336392948345:layer:AWSSDKPandas-Python39:20'

Resources:
  # Redshift Cluster
  RedshiftClusterParameterGroup:
    Type: AWS::Redshift::ClusterParameterGroup
    Properties:
      Description: Custom parameter group for Redshift cluster
      ParameterGroupFamily: redshift-1.0
      Parameters:
        - ParameterName: enable_user_activity_logging
          ParameterValue: "true"
        - ParameterName: require_ssl
          ParameterValue: "true"

  RedshiftCluster:
    Type: AWS::Redshift::Cluster
    Properties:
      DBName: !Ref RedshiftDatabaseName
      ClusterIdentifier: biomarker-redshift-cluster
      NodeType: ra3.large
      MasterUsername: !Ref RedshiftUserName
      MasterUserPassword: !Ref RedshiftPassword
      ClusterType: single-node
      PubliclyAccessible: false
      VpcSecurityGroupIds: [!Ref SecurityGroup]
      ClusterSubnetGroupName: !Ref RedshiftSubnetGroup
      ClusterParameterGroupName: !Ref RedshiftClusterParameterGroup
      Encrypted: true
      LoggingProperties:
        BucketName: !Ref S3Bucket
        S3KeyPrefix: 'redshift-logs/'
        LogDestinationType: s3
      IamRoles: 
        - !GetAtt RedshiftRole.Arn
  
  RedshiftRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: redshift.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
      Policies:
        - PolicyName: RedshiftS3LoggingPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetBucketLocation
                  - s3:ListBucket
                  - s3:GetBucketAcl
                  - s3:PutBucketAcl
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
      

  # Glue Connection
  JDBCConnection:
    Type: AWS::Glue::Connection
    Properties:
      CatalogId: !Ref 'AWS::AccountId'
      ConnectionInput:
        Name: jdbc_connector_biomarkers
        Description: JDBC connection for Redshift
        ConnectionType: JDBC
        ConnectionProperties:
          JDBC_CONNECTION_URL: !Sub 'jdbc:redshift://${RedshiftCluster.Endpoint.Address}:5439/${RedshiftDatabaseName}'
          USERNAME: !Ref RedshiftUserName
          PASSWORD: !Ref RedshiftPassword
        PhysicalConnectionRequirements:
          SubnetId: !Ref PrivateSubnet1
          SecurityGroupIdList: [!Ref SecurityGroup]
          AvailabilityZone: !Select 
            - 0
            - !GetAZs 
              Ref: 'AWS::Region'

  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for Redshift cluster
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 5439
          ToPort: 5439
          CidrIp: 10.0.0.0/16  # Only allow access from within VPC
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-RedshiftSG

  RedshiftSubnetGroup:
    Type: AWS::Redshift::ClusterSubnetGroup
    Properties:
      Description: Subnet group for Redshift cluster
      SubnetIds:
        - !Ref PrivateSubnet1
        - !Ref PrivateSubnet2
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-RedshiftSubnetGroup

  # Bedrock AgentCore runtime role
  BedrockAgentCoreStrandsExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - bedrock-agentcore.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonBedrockFullAccess"
        - "arn:aws:iam::aws:policy/AWSLambda_FullAccess"
        - "arn:aws:iam::aws:policy/AmazonS3FullAccess"
        - "arn:aws:iam::aws:policy/AmazonRedshiftQueryEditor"
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess"
      Policies:
        - PolicyName: AgentCorePolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - states:*
                Resource: "*"
              - Effect: Allow
                Action:
                  - logs:DescribeLogStreams
                  - logs:CreateLogGroup
                Resource:
                  - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/bedrock-agentcore/runtimes/*
              - Effect: Allow
                Action:
                  - logs:DescribeLogGroups
                Resource:
                  - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:*
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/bedrock-agentcore/runtimes/*:log-stream:*
              - Effect: Allow
                Action:
                  - xray:PutTraceSegments
                  - xray:PutTelemetryRecords
                  - xray:GetSamplingRules
                  - xray:GetSamplingTargets
                Resource:
                  - "*"
              - Effect: Allow
                Resource: "*"
                Action: cloudwatch:PutMetricData
                Condition:
                  StringEquals:
                    cloudwatch:namespace: bedrock-agentcore
              - Effect: Allow
                Resource: "*"
                Action: s3:GetObject
              - Effect: Allow
                Resource: "*"
                Action: lambda:InvokeFunction
              - Effect: Allow
                Action:
                  - bedrock-agentcore:*
                  - iam:PassRole
                Resource: "*"
              - Sid: GetAgentAccessToken
                Effect: Allow
                Action:
                  - bedrock-agentcore:GetWorkloadAccessToken
                  - bedrock-agentcore:GetWorkloadAccessTokenForJWT
                  - bedrock-agentcore:GetWorkloadAccessTokenForUserId
                Resource:
                  - !Sub arn:aws:bedrock-agentcore:${AWS::Region}:${AWS::AccountId}:workload-identity-directory/default
                  - !Sub arn:aws:bedrock-agentcore:${AWS::Region}:${AWS::AccountId}:workload-identity-directory/default/workload-identity/agentcore_strands-*

  # VPC
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-VPC
  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      AvailabilityZone: !Select 
        - 0
        - !GetAZs 
          Ref: 'AWS::Region'
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-PrivateSubnet1
  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.2.0/24
      AvailabilityZone: !Select 
        - 1
        - !GetAZs 
          Ref: 'AWS::Region'
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-PrivateSubnet2
  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.3.0/24
      AvailabilityZone: !Select 
        - 0
        - !GetAZs 
          Ref: 'AWS::Region'
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-PublicSubnet1
  NatGatewayEIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: vpc
  NatGateway:
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId: !GetAtt NatGatewayEIP.AllocationId
      SubnetId: !Ref PublicSubnet1
  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-PrivateRouteTable

  PrivateRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway

  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnet1
      RouteTableId: !Ref PrivateRouteTable

  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnet2
      RouteTableId: !Ref PrivateRouteTable

  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName}-PublicRouteTable

  PublicRoute:
    Type: AWS::EC2::Route
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet1
      RouteTableId: !Ref PublicRouteTable


  # Internet Gateway
  InternetGateway:
    Type: AWS::EC2::InternetGateway

  # VPC Gateway Attachment
  VPCGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway
  
  # S3
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "${EnvironmentName}-${AWS::AccountId}-${AWS::Region}-agent-bucket"
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
  S3BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref S3Bucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowCloudFormationReadAccess
            Effect: Allow
            Principal:
              Service: cloudformation.amazonaws.com
            Action:
              - s3:GetObject
            Resource: !Sub arn:aws:s3:::${S3Bucket}/*
          - Sid: AllowCodeBuildAndLambdaAccess
            Effect: Allow
            Principal:
              AWS: 
                - !GetAtt CodeBuildServiceRole.Arn
            Action:
              - s3:PutObject
              - s3:GetObject
              - s3:ListBucket
            Resource:
              - !Sub arn:aws:s3:::${S3Bucket}
              - !Sub arn:aws:s3:::${S3Bucket}/*
          - Sid: AllowRedshiftLogging
            Effect: Allow
            Principal:
              Service: redshift.amazonaws.com
            Action:
              - s3:PutObject
              - s3:GetBucketLocation
              - s3:ListBucket
              - s3:GetBucketAcl
              - s3:PutBucketAcl
            Resource:
              - !Sub arn:aws:s3:::${S3Bucket}
              - !Sub arn:aws:s3:::${S3Bucket}/*
  S3DataProcessingLambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:CreateBucket'
                  - 's3:PutObject'
                Resource: 
                  - !Sub 'arn:aws:s3:::biomarkers-discovery-agent-test-${AWS::Region}-${AWS::AccountId}'
                  - !Sub 'arn:aws:s3:::biomarkers-discovery-agent-test-${AWS::Region}-${AWS::AccountId}/*'
        - PolicyName: STSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sts:GetCallerIdentity'
                Resource: '*'
  S3DataProcessingLambda:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt S3DataProcessingLambdaRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import pandas as pd
          import gzip
          import requests
          import shutil
          import os
          import io
          from io import StringIO
          import urllib3

          http = urllib3.PoolManager()

          def send_response(event, context, response_status, response_data, physical_resource_id=None):
              response_url = event['ResponseURL']

              response_body = {
                'Status': response_status,
                'Reason': f"See the details in CloudWatch Log Stream: {context.log_stream_name}",
                'PhysicalResourceId': physical_resource_id or context.log_stream_name,
                'StackId': event['StackId'],
                'RequestId': event['RequestId'],
                'LogicalResourceId': event['LogicalResourceId'],
                'Data': response_data
              }

              json_response_body = json.dumps(response_body)

              headers = {
                'content-type': '',
                'content-length': str(len(json_response_body))
              }

              response = http.request('PUT', response_url, body=json_response_body, headers=headers)
              return response.status

          def lambda_handler(event, context):
              try:
                  if event['RequestType'] == 'Delete':
                      send_response(event, context, 'SUCCESS', {})
                      return

                  s3 = boto3.client('s3')
                  session = boto3.session.Session()
                  region = session.region_name
                  accountID = boto3.client('sts').get_caller_identity().get('Account')

                  setup_Bucket_Name = f'biomarkers-discovery-agent-test-{region}-{accountID}'
                  solution_name = 'biomarkers'
                  input_data_bucket = f"s3://{setup_Bucket_Name}/data/{solution_name}"
                  SOLUTION_PREFIX = 'multi-modal'
                  BUCKET = setup_Bucket_Name

                  genomic_dataset_url = "https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE103584&format=file&file=GSE103584%5FR01%5FNSCLC%5FRNAseq%2Etxt%2Egz"
                  clinical_dataset_url = "https://www.cancerimagingarchive.net/wp-content/uploads/NSCLCR01Radiogenomic_DATA_LABELS_2018-05-22_1500-shifted.csv"
                  genomic_dataset_name = "GSE103584_R01_NSCLC_RNAseq.txt"
                  clinical_dataset_name = "NSCLCR01Radiogenomic_DATA_LABELS_2018-05-22_1500-shifted.csv"

                  # Create S3 bucket for data
                  try:
                      if region != 'us-east-1':
                          s3.create_bucket(Bucket=setup_Bucket_Name, CreateBucketConfiguration={
                              'LocationConstraint': region})
                      else:
                          s3.create_bucket(Bucket=setup_Bucket_Name)
                  except s3.exceptions.BucketAlreadyExists:
                      print(f"Bucket {setup_Bucket_Name} already exists")
                  except s3.exceptions.BucketAlreadyOwnedByYou:
                      print(f"Bucket {setup_Bucket_Name} already owned by you")

                  # Download and process genomic data
                  genomic_response = requests.get(genomic_dataset_url)
                  if genomic_response.status_code == 200:
                      with gzip.open(io.BytesIO(genomic_response.content), 'rt') as f:
                          gen_data = pd.read_csv(f, delimiter='\t')
                      s3.put_object(Body=gen_data.to_csv(index=False, sep='\t'), Bucket=setup_Bucket_Name, Key=f'data/{solution_name}/genomics/{genomic_dataset_name}')
                  else:
                      raise Exception('Failed to download genomic data')

                  # Download and process clinical data
                  clinical_response = requests.get(clinical_dataset_url)
                  if clinical_response.status_code == 200:
                      data_clinical = pd.read_csv(StringIO(clinical_response.text))
                      s3.put_object(Body=data_clinical.to_csv(index=False), Bucket=setup_Bucket_Name, Key=f'data/{solution_name}/clinical/{clinical_dataset_name}')
                  else:
                      raise Exception('Failed to download clinical data')
 
                  # Process the data
                  data_clinical = data_clinical[~data_clinical["Case ID"].str.contains("AMC")]

                  list_delete_cols = ['Quit Smoking Year', 'Date of Recurrence', 'Date of Last Known Alive', 'Date of Death', 'CT Date', 'PET Date']
                  data_clinical.drop(list_delete_cols, axis=1, inplace=True)

                  data_clinical["Survival Status"].replace({"Dead": "1", "Alive": "0"}, inplace=True)

                  data_clinical.fillna(0, inplace=True)

                  drop_cases = ['R01-003', 'R01-004', 'R01-006', 'R01-007', 'R01-015', 'R01-016', 'R01-018', 'R01-022', 'R01-023', 'R01-098', 'R01-105']
                  gen_data = gen_data.drop(drop_cases, axis=1)

                  gen_data.rename(columns={'Unnamed: 0':'index_temp'}, inplace=True)
                  gen_data.set_index('index_temp', inplace=True)
                  gen_data_t = gen_data.transpose()
                  gen_data_t.reset_index(inplace=True)
                  gen_data_t.rename(columns={'index':'Case_ID'}, inplace=True)
                  
                  selected_columns = ['Case_ID','LRIG1', 'HPGD', 'GDF15', 'CDH2', 'POSTN', 'VCAN', 'PDGFRA', 'VCAM1', 'CD44', 'CD48', 'CD4', 'LYL1', 'SPI1', 'CD37', 'VIM', 'LMO2', 'EGR2', 'BGN', 'COL4A1', 'COL5A1', 'COL5A2']
                  gen_data_t = gen_data_t[selected_columns]

                  data_gen = gen_data_t.fillna(0)
                  data_clinical = data_clinical.rename(columns={'Case ID': 'Case_ID'}) 
                  data_clinical[['Time to Death (days)']] = data_clinical[['Time to Death (days)']].fillna(value=3000)
                  inner_merged_total = pd.merge(data_gen, data_clinical, on=["Case_ID"])
                  inner_merged_total['Survival duration'] = inner_merged_total['Age at Histological Diagnosis'] + inner_merged_total['Time to Death (days)']/365
                  # Save the final result
                  clinical_genomic_key = f'data/{solution_name}/clinical_genomic.csv'
                  s3.put_object(Body=inner_merged_total.to_csv(index=False), Bucket=setup_Bucket_Name, Key=clinical_genomic_key)
                  

                  # Save the final result
                  s3.put_object(Body=inner_merged_total.to_csv(index=False), Bucket=setup_Bucket_Name, Key=f'data/{solution_name}/clinical_genomic.csv')
                  chemotherapy_cases = inner_merged_total[inner_merged_total['Chemotherapy'] == 'Yes'].copy()
                  chemotherapy_cases.loc[chemotherapy_cases['LRIG1'] <= 25, 'ExpressionGroup'] = 0
                  chemotherapy_cases.loc[chemotherapy_cases['LRIG1'] > 25, 'ExpressionGroup'] = 1
                  
                  chemotherapy_cases = chemotherapy_cases[['LRIG1', 'Survival Status', 'Survival duration', 'ExpressionGroup']]
                  
                  # Save chemotherapy survival data
                  s3.put_object(Body=chemotherapy_cases.to_csv(index=False), Bucket=setup_Bucket_Name, Key=f'data/{solution_name}/chemotherapy_survival.csv')
                  chemotherapy_survival_key = f'data/{solution_name}/chemotherapy_survival.csv'



                  send_response(event, context, 'SUCCESS', {
                  'BucketName': setup_Bucket_Name,
                  'ClinicalGenomicKey': clinical_genomic_key,
                  'ChemotherapySurvivalKey': chemotherapy_survival_key
              })
              except Exception as e:
                  print(e)
                  send_response(event, context, 'FAILED', {'Error': str(e)})

      Runtime: python3.9
      Timeout: 900
      MemorySize: 3008
      Layers:
        - !FindInMap [RegionMap, !Ref 'AWS::Region', PandasLayer]
  S3DataProcessingCustomResource:
    Type: 'Custom::S3DataProcessing'
    Properties:
      ServiceToken: !GetAtt S3DataProcessingLambda.Arn
  
  # Lambda Function to Load Data
  LambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties: 
      AssumeRolePolicyDocument: 
        Version: '2012-10-17'
        Statement: 
          - Effect: 'Allow'
            Principal: 
              Service: 
                - 'lambda.amazonaws.com'
            Action: 
              - 'sts:AssumeRole'
      Policies: 
        - PolicyName: 'LambdaExecutionPolicy'
          PolicyDocument: 
            Version: '2012-10-17'
            Statement: 
              - Effect: 'Allow'
                Action: 
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 
                  - 'arn:aws:logs:*:*:*'
              - Effect: 'Allow'
                Action: 
                  - 's3:GetObject'
                  - 's3:ListBucket'
                  - 's3:HeadObject'
                  - 's3:GetObjectAttributes'
                Resource: 
                  - !Sub 'arn:aws:s3:::biomarkers-discovery-agent-test-${AWS::Region}-${AWS::AccountId}'
                  - !Sub 'arn:aws:s3:::biomarkers-discovery-agent-test-${AWS::Region}-${AWS::AccountId}/*'
              - Effect: 'Allow'
                Action: 
                  - 'redshift-data:*'
                  - 'redshift:*'
                Resource: 
                  - !Sub 'arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbuser:biomarker-redshift-cluster/*'
                  - !Sub 'arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbuser:biomarker-redshift-cluster'
              - Effect: 'Allow'
                Action: 
                  - 'ec2:CreateNetworkInterface'
                  - 'ec2:DeleteNetworkInterface'
                  - 'ec2:DescribeNetworkInterfaces'
                Resource: 
                  - !Sub 'arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:network-interface/*'
              - Effect: 'Allow'
                Action:
                  - 'ec2:DescribeSecurityGroups'
                  - 'ec2:DescribeSubnets'
                  - 'ec2:DescribeVpcs'
                Resource:
                  - !Sub 'arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:security-group/${SecurityGroup}'
                  - !Sub 'arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/${PrivateSubnet1}'
                  - !Sub 'arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/${PrivateSubnet2}'
                  - !Sub 'arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:vpc/${VPC}'
              - Effect: 'Allow'
                Action: 
                  - 'redshift:*'
                  - 'redshift-data:*'
                Resource: 
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:cluster:biomarker-redshift-cluster
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbname:biomarker-redshift-cluster/dev 
  LambdaFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: 'index.lambda_handler'
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import csv
          import urllib3
          import os
          import redshift_connector
          import pandas as pd
          import time
          print("hello")

          s3_client = boto3.client('s3')
          redshift_data = boto3.client('redshift-data')
          http = urllib3.PoolManager()
          database = os.environ['REDSHIFT_DATABASE']
          cluster_id = os.environ['REDSHIFT_CLUSTER_ID']
          DbUser = os.environ['REDSHIFT_USER']
         


          def send_response(event, context, response_status, response_data, physical_resource_id=None):
            response_url = event['ResponseURL']

            response_body = {
              'Status': response_status,
              'Reason': f"See the details in CloudWatch Log Stream: {context.log_stream_name}",
              'PhysicalResourceId': physical_resource_id or context.log_stream_name,
              'StackId': event['StackId'],
              'RequestId': event['RequestId'],
              'LogicalResourceId': event['LogicalResourceId'],
              'Data': response_data
            }

            json_response_body = json.dumps(response_body)

            headers = {
              'content-type': '',
              'content-length': str(len(json_response_body))
            }

            response = http.request('PUT', response_url, body=json_response_body, headers=headers)
            return response.status


          def create_chemotherapy_survival_table(cluster_id, database, Dbuser):
            sql = f"""
              CREATE TABLE IF NOT EXISTS "dev"."public"."chemotherapy_survival" (
              LRIG1 FLOAT,
              Survival_Status BOOLEAN,
              Survival_Duration FLOAT,
              ExpressionGroup FLOAT
          );

              COMMENT ON COLUMN "dev"."public"."chemotherapy_survival".LRIG1 IS 'Gene expression value for LRIG1';
              COMMENT ON COLUMN "dev"."public"."chemotherapy_survival".Survival_Status IS 'BOOLEAN 0 for Alive or 1 for Dead';
              COMMENT ON COLUMN "dev"."public"."chemotherapy_survival".Survival_Duration IS 'Survival duration in days';
              COMMENT ON COLUMN "dev"."public"."chemotherapy_survival".ExpressionGroup IS 'Integer 1 or 0 for expression group';
            """
            response = redshift_data.execute_statement(
                ClusterIdentifier=cluster_id,
                Database=database,
                DbUser=Dbuser,
                Sql=sql
            )
            return response

          def create_clinical_genomic_table(cluster_id, database, Dbuser):
            sql = """
            CREATE TABLE IF NOT EXISTS "dev"."public"."clinical_genomic"(
              Case_ID VARCHAR(50),
              LRIG1 FLOAT,
              HPGD FLOAT,
              GDF15 FLOAT,
              CDH2 FLOAT,
              POSTN FLOAT,
              VCAN FLOAT,
              PDGFRA FLOAT,
              VCAM1 FLOAT,
              CD44 FLOAT,
              CD48 FLOAT,
              CD4 FLOAT,
              LYL1 FLOAT,
              SPI1 FLOAT,
              CD37 FLOAT,
              VIM FLOAT,
              LMO2 FLOAT,
              EGR2 FLOAT,
              BGN FLOAT,
              COL4A1 FLOAT,
              COL5A1 FLOAT,
              COL5A2 FLOAT,
              Patient_affiliation VARCHAR(50),
              Age_at_Histological_Diagnosis INT,
              Weight_lbs FLOAT,
              Gender VARCHAR(10),
              Ethnicity VARCHAR(50),
              Smoking_status VARCHAR(50),
              Pack_Years INT,
              Percent_GG VARCHAR(20),
              Tumor_Location_RUL VARCHAR(20),
              Tumor_Location_RML VARCHAR(20),
              Tumor_Location_RLL VARCHAR(20),
              Tumor_Location_LUL VARCHAR(20),
              Tumor_Location_LLL VARCHAR(20),
              Tumor_Location_L_Lingula VARCHAR(20),
              Tumor_Location_Unknown VARCHAR(20),
              Histology VARCHAR(70),
              Pathological_T_stage VARCHAR(20),
              Pathological_N_stage VARCHAR(20),
              Pathological_M_stage VARCHAR(20),
              Histopathological_Grade VARCHAR(70),
              Lymphovascular_invasion VARCHAR(60),
              Pleural_invasion VARCHAR(50),
              EGFR_mutation_status VARCHAR(50),
              KRAS_mutation_status VARCHAR(50),
              ALK_translocation_status VARCHAR(50),
              Adjuvant_Treatment VARCHAR(20),
              Chemotherapy VARCHAR(20),
              Radiation VARCHAR(20),
              Recurrence VARCHAR(20),
              Recurrence_Location VARCHAR(50),
              Survival_Status BOOLEAN,
              Time_to_Death FLOAT,
              Days_between_CT_and_surgery INT,
              Survival_Duration FLOAT
              
              
            );

              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Case_ID IS 'Unique identifier for each case';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".LRIG1 IS 'Gene expression value for LRIG1';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".HPGD IS 'Gene expression value for HPGD';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".GDF15 IS 'Gene expression value for GDF15';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".CDH2 IS 'Gene expression value for CDH2';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".POSTN IS 'Gene expression value for POSTN';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".VCAN IS 'Gene expression value for VCAN';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".PDGFRA IS 'Gene expression value for PDGFRA';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".VCAM1 IS 'Gene expression value for VCAM1';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".CD44 IS 'Gene expression value for CD44';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".CD48 IS 'Gene expression value for CD48';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".CD4 IS 'Gene expression value for CD4';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".LYL1 IS 'Gene expression value for LYL1';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".SPI1 IS 'Gene expression value for SPI1';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".CD37 IS 'Gene expression value for CD37';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".VIM IS 'Gene expression value for VIM';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".LMO2 IS 'Gene expression value for LMO2';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".EGR2 IS 'Gene expression value for EGR2';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".BGN IS 'Gene expression value for BGN';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".COL4A1 IS 'Gene expression value for COL4A1';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".COL5A1 IS 'Gene expression value for COL5A1';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".COL5A2 IS 'Gene expression value for COL5A2';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Patient_affiliation IS 'VA or Stanford';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Age_at_Histological_Diagnosis IS 'Age at Histological Diagnosis';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Weight_lbs IS 'Weight in pounds';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Gender IS 'Male or Female';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Ethnicity IS 'Ethnicity';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Smoking_status IS 'Current, Former, or Never';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Pack_Years IS 'Number of pack years for smokers';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Percent_GG IS 'Percentage of ground glass opacity (GG) in the tumor';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_RUL IS 'Right Upper Lobe';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_RML IS 'Right Middle Lobe';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_RLL IS 'Right Lower Lobe';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_LUL IS 'Left Upper Lobe';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_LLL IS 'Left Lower Lobe';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_L_Lingula IS 'Left Lingula';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Tumor_Location_Unknown IS 'Unknown location';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Histology IS 'Histology type';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Pathological_T_stage IS 'Pathological T stage';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Pathological_N_stage IS 'Pathological N stage';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Pathological_M_stage IS 'Pathological M stage';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Histopathological_Grade IS 'G1 Well differentiated, G2 Moderately differentiated, G3 Poorly differentiated';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Lymphovascular_invasion IS 'Present or Absent';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Pleural_invasion IS 'Yes or No';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".EGFR_mutation_status IS 'Mutant, Wildtype, or Unknown';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".KRAS_mutation_status IS 'Mutant, Wildtype, or Unknown';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".ALK_translocation_status IS 'Positive, Negative, or Unknown';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Adjuvant_Treatment IS 'Yes or No';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Chemotherapy IS 'Yes or No';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Radiation IS 'Yes or No';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Recurrence IS 'Yes or No';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Recurrence_Location IS 'Local, Distant, or N/A';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Survival_Status IS 'BOOLEAN 0 represents Alive and 1 represent Dead';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Time_to_Death IS 'Time to death in days, or 0 if alive';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Days_between_CT_and_surgery IS 'Number of days between CT scan and surgery';
              COMMENT ON COLUMN "dev"."public"."clinical_genomic".Survival_Duration IS 'duration of paitent survial in years';
            """
            
            response = redshift_data.execute_statement(
                ClusterIdentifier=cluster_id,
                Database=database,
                DbUser=Dbuser,
                Sql=sql
            )
            print("data loaded")
            return response

          def load_chemotherapy_data(cluster_id, database, Dbuser, df):
            
            for row in df.itertuples():
              sql = f"""
              INSERT INTO "dev"."public"."chemotherapy_survival" (
                LRIG1,
                Survival_Status,
                Survival_Duration,
                ExpressionGroup)
                VALUES ('{row[1]}', '{row[2]}', '{row[3]}', '{row[4]}');"""
                
              response = redshift_data.execute_statement(
                    ClusterIdentifier=cluster_id,
                    Database=database,
                    DbUser=Dbuser,
                    Sql=sql
                )

            # Access specific attributes of the response
            if 'ResponseMetadata' in response:
                print("HTTP Status Code:", response['ResponseMetadata']['HTTPStatusCode'])
                print("Request ID:", response['ResponseMetadata']['RequestId'])

            if 'Error' in response:
                print("Error:", response['Error'])

            return response

          def load_clinical_genomic_data(cluster_id, database, Dbuser, df):
            
            for row in df.itertuples():
              sql = f"""INSERT INTO "dev"."public"."clinical_genomic" (
              Case_ID, LRIG1, HPGD, GDF15, CDH2, POSTN, VCAN,
              PDGFRA, VCAM1, CD44, CD48, CD4, LYL1, SPI1, CD37,
              VIM, LMO2, EGR2, BGN, COL4A1, COL5A1, COL5A2, 
              Patient_affiliation, Age_at_Histological_Diagnosis, 
              Weight_lbs, Gender, Ethnicity, Smoking_status,
              Pack_Years, Percent_GG, Tumor_Location_RUL,
              Tumor_Location_RML, Tumor_Location_RLL,
              Tumor_Location_LUL, Tumor_Location_LLL,
              Tumor_Location_L_Lingula, Tumor_Location_Unknown,
              Histology, Pathological_T_stage, Pathological_N_stage,
              Pathological_M_stage, Histopathological_Grade,
              Lymphovascular_invasion, Pleural_invasion,
              EGFR_mutation_status, KRAS_mutation_status,
              ALK_translocation_status, Adjuvant_Treatment, Chemotherapy,
              Radiation, Recurrence, Recurrence_Location,
              Survival_Status, Time_to_Death,
              Days_between_CT_and_surgery, Survival_Duration)
              VALUES (
              {','.join([f"'{row[i]}'" for i in range(1, len(row))])});"""
              print(sql)
                
              response = redshift_data.execute_statement(
                    ClusterIdentifier=cluster_id,
                    Database=database,
                    DbUser=Dbuser,
                    Sql=sql
                  )

            if 'ResponseMetadata' in response:
                print("HTTP Status Code:", response['ResponseMetadata']['HTTPStatusCode'])
                print("Request ID:", response['ResponseMetadata']['RequestId'])

            if 'Error' in response:
                print("Error:", response['Error'])
            return response
          
          def lambda_handler(event, context):
            try:
                if event['RequestType'] == 'Delete':
                    send_response(event, context, 'SUCCESS', {})
                    return
    
                s3_bucket = event['ResourceProperties']['S3Bucket']
                s3_key_clinical_genomic_data = event['ResourceProperties']['ClinicalGenomicKey']
                s3_key_chemotherapy_survival_data = event['ResourceProperties']['ChemotherapySurvivalKey']
            
                #Create chemotherapy table
                create_chemotherapy_survival_table(cluster_id, database, DbUser)
                time.sleep(5)
                
                #download data
                with open('/tmp/chemotherapy_survival.csv', 'wb') as file:
                    s3_client.download_fileobj(s3_bucket, s3_key_chemotherapy_survival_data, file)
                chemotherapy_df = pd.read_csv('/tmp/chemotherapy_survival.csv')
                
                #Load chemotherapy data to table
                load_chemotherapy_data(cluster_id, database, DbUser, chemotherapy_df)
                time.sleep(5)
              
                #Create clinical genomic table
                create_clinical_genomic_table(cluster_id, database, DbUser)
                time.sleep(5)
                
                #download data
                with open('/tmp/clinical_genomic.csv', 'wb') as file:
                    s3_client.download_fileobj(s3_bucket, s3_key_clinical_genomic_data, file)
                clinical_df = pd.read_csv('/tmp/clinical_genomic.csv')
                
                # Load clinical genomic data to table
                load_clinical_genomic_data(cluster_id, database, DbUser, clinical_df)
                time.sleep(5)

                send_response(event, context, 'SUCCESS', {'Status': 'Success'})

            except Exception as e:
                send_response(event, context, 'FAILED', {'Status': 'Failure', 'Error': str(e)})
                print(e)
      Runtime: 'python3.9'
      Timeout: 900 
      MemorySize: 512
      Layers:
        - !FindInMap [RegionMap, !Ref 'AWS::Region', PandasLayer]
      Environment:
        Variables:
          REDSHIFT_CLUSTER_ID: 'biomarker-redshift-cluster'
          REDSHIFT_DATABASE: !Ref RedshiftDatabaseName
          REDSHIFT_USER: 'admin'

  LambdaFunctionInvokePermission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      Action: 'lambda:InvokeFunction'
      Principal: 'cloudformation.amazonaws.com'
      FunctionName: !GetAtt LambdaFunction.Arn

  # Custom Resource to Load Data to Redshift
  LoadDataToRedshift:
    DependsOn: 
      - RedshiftCluster
      - S3DataProcessingCustomResource
    Type: Custom::LoadDataToRedshift
    Properties:
      ServiceToken: !GetAtt LambdaFunction.Arn
      S3Bucket: !GetAtt S3DataProcessingCustomResource.BucketName
      ClinicalGenomicKey: !GetAtt S3DataProcessingCustomResource.ClinicalGenomicKey
      ChemotherapySurvivalKey: !GetAtt S3DataProcessingCustomResource.ChemotherapySurvivalKey
  
  # Create ECR repository
  ECRRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: lifelines-lambda-sample

  CleanupLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt CleanupLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse

          def delete_bucket_contents(bucket):
              s3 = boto3.resource('s3')
              bucket = s3.Bucket(bucket)
              bucket.objects.all().delete()

          def handler(event, context):
              if event['RequestType'] == 'Delete':
                  s3 = boto3.client('s3')
                  bucket = event['ResourceProperties']['S3Bucket']
                  try:
                      # Delete all objects in the bucket
                      delete_bucket_contents(bucket)
                      print(f"Deleted all contents from bucket: {bucket}")
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  except Exception as e:
                      print(f"Error: {str(e)}")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {"Error": str(e)})
              else:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
      Runtime: python3.8
      Timeout: 300

  CleanupLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: S3DeletePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:DeleteObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
                  - !Sub arn:aws:s3:::${LogBucket}
                  - !Sub arn:aws:s3:::${LogBucket}/*
        - PolicyName: CloudWatchLogsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: arn:aws:logs:*:*:*

  CleanupCustomResource:
    Type: Custom::Cleanup
    Properties:
      ServiceToken: !GetAtt CleanupLambdaFunction.Arn
      S3Bucket: !Ref S3Bucket
      S3Prefix: ''

  CleanupCustomLogResource:
    Type: Custom::Cleanup
    Properties:
      ServiceToken: !GetAtt CleanupLambdaFunction.Arn
      S3Bucket: !Ref LogBucket
      S3Prefix: ''

  LogBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      BucketName: !Sub "${EnvironmentName}-${AWS::AccountId}-${AWS::Region}-log-bucket"

  LogBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref LogBucket
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Sid: AllowLogDeliveryWrite
            Effect: Allow
            Principal:
              Service: logging.s3.amazonaws.com
            Action:
              - s3:PutObject
            Resource: !Sub "${LogBucket.Arn}/*"

  # Create CloudWatch Log Group
  CodeBuildLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/codebuild/${AWS::StackName}-DockerPushProject"
      RetentionInDays: 14

  # Gather Assets for pubmed lambda function, knowledgebase and survival data processing lambda
  DataRetrievalLogic:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: DataRetrievalLogic
      ServiceRole: !GetAtt DataRetrievalRole.Arn
      Artifacts:
        Type: NO_ARTIFACTS
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref CodeBuildLogGroup
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        PrivilegedMode: true
      Source:
        Type: GITHUB
        Location: !Ref GithubLink
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - yum install -y git-lfs
                - git lfs install
                - echo "downloading Ncbi article for KB ingestion"
                - yum install -y wget
                - pip install --upgrade pip
               
            build:
              commands:
                - echo "Starting build phase"
                - echo "Downloading NCBI article..."
                - wget --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36" https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5749594/pdf/radiol.2017161845.pdf -O ncbi_article.pdf
                - echo "Downloading NCBI article from S3 to local..."
                - aws s3 cp s3://aws-hcls-ml/literature/ncbi_article.pdf ncbi_article_s3.pdf
                - echo "Uploading NCBI article to destination S3 bucket..."
                - aws s3 cp ncbi_article_s3.pdf s3://${S3Bucket}/ncbi_article.pdf
                - echo "NCBI article uploaded to S3"
                - echo "Cloning Git repository..."
                - GIT_LFS_SKIP_SMUDGE=1 git clone -b main --single-branch ${GithubLink} repo
                - echo "Zipping Lambda function..."
                - cd repo
                - cd multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/matplotbarchartlambda
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - zip -r matplotbarchartlambda.zip $items_to_zip
                - cd ../../../../..
                - echo "Moving zip file to project root..."
                - cp multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/matplotbarchartlambda/matplotbarchartlambda.zip ../
                - cd ..
                - aws s3 cp matplotbarchartlambda.zip s3://${S3Bucket}/matplotbarchartlambda.zip
                - cd repo
                - cd multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/pubmed-lambda-function
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - echo "Creating zip pubmed lambda file..."
                - zip -r pubmed-lambda-function.zip $items_to_zip
                - echo "Lambda function zipped"
                - cd ../../../../..
                - echo "Moving zip file to project root..."
                - cp multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/pubmed-lambda-function/pubmed-lambda-function.zip ../
                - cd ..
                - aws s3 cp pubmed-lambda-function.zip s3://${S3Bucket}/pubmed-lambda-function.zip
                - cd repo
                - cd multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/querydatabaselambda
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - zip -r querydatabaselambda.zip $items_to_zip
                - cd ../../../../..
                - echo "Moving zip file to project root..."
                - cp multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/querydatabaselambda/querydatabaselambda.zip ../
                - cd ..
                - aws s3 cp querydatabaselambda.zip s3://${S3Bucket}/querydatabaselambda.zip
                - cd repo
                - cd multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/survivaldataprocessinglambda
                - echo "Creating list of items to zip..."
                - items_to_zip=$(ls -A | tr '\n' ' ')
                - zip -r survivaldataprocessinglambda.zip $items_to_zip
                - cd ../../../../..
                - echo "Moving zip file to project root..."
                - cp multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/survivaldataprocessinglambda/survivaldataprocessinglambda.zip ../
                - cd ..
                - aws s3 cp survivaldataprocessinglambda.zip s3://${S3Bucket}/survivaldataprocessinglambda.zip
                - echo "Build phase completed"
                - echo "lambda layer preperation phase starts"
                - cd repo
                - mkdir -p python
                - pip install -r multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/lambdalayers/requirements.txt -t python
                - cd python
                - zip -r9 ../lambda-layer.zip .
                - aws s3 cp ../lambda-layer.zip s3://${S3Bucket}/lambda-layer.zip

      TimeoutInMinutes: 10

  # Log into ECR and push scientific-plots-with-lifelines Docker image
  DockerPushLogic:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: DockerPushProject
      ServiceRole: !GetAtt DockerPushRole.Arn
      Artifacts:
        Type: NO_ARTIFACTS
      LogsConfig:
        CloudWatchLogs:
          Status: ENABLED
          GroupName: !Ref CodeBuildLogGroup
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        PrivilegedMode: true
      Source:
        Type: GITHUB
        Location: !Ref GithubLink
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - yum install -y git-lfs
                - git lfs install
                - echo "Starting pre_build phase"
                - echo "Logging into Amazon ECR..."
                - aws ecr get-login-password --region ${AWS::Region} | docker login --username AWS --password-stdin ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
                - echo "ECR login complete"
            build:
              commands:
                - echo "Starting build phase"
                - echo "Cloning Git repository..."
                - GIT_LFS_SKIP_SMUDGE=1 git clone -b main --single-branch ${GithubLink} repo
                - cd repo/multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/scientific-plots-with-lifelines
                - echo "Building Docker image..."
                - docker build -t lifelines-python3.12-v2 .
                - echo "Tagging Docker image..."
                - docker tag lifelines-python3.12-v2:latest ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
                - echo "Docker image tagged"
            post_build:
              commands:
                - echo "Starting post_build phase"
                - echo "Pushing Docker image to ECR..."
                - docker push ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
                - echo "Docker image pushed to ECR"
                - echo "Displaying Docker images"
                - docker images
                - echo "Displaying ECR repository contents"
                - aws ecr list-images --repository-name ${ECRRepository}
      TimeoutInMinutes: 10

  ImagingECRRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName: medical-image-processing-smstudio

  ImagingDockerBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:3.0
        Type: LINUX_CONTAINER
        PrivilegedMode: true
      ServiceRole: !GetAtt ImagingDockerBuildRole.Arn
      Source:
        Type: GITHUB
        Location: !Ref GithubLink
        BuildSpec: !Sub |
          version: 0.2
          phases:
            pre_build:
              commands:
                - echo Logging in to Amazon ECR...
                - aws ecr get-login-password --region ${AWS::Region} | docker login --username AWS --password-stdin ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com
                - echo Cloning the repository...
                - git clone -b main --single-branch ${GithubLink} repo
                - cd repo/multi_agent_collaboration/cancer_biomarker_discovery/bedrock_agents/ActionGroups/imaging-biomarker 
                - echo Checking for required files...
                - ls -la
                - if [ ! -f requirements.txt ] || [ ! -f dcm2nifti_processing.py ] || [ ! -f radiomics_utils.py ]; then echo "Missing required files"; exit 1; fi
                - zip -r Imaginglambdafunction.zip dummy_lambda.py
                - echo Copying lambda function 
                - aws s3 cp Imaginglambdafunction.zip s3://${S3Bucket}/Imaginglambdafunction.zip
               

            build:
              commands:
                - echo Build started on `date`
                - echo Building the Docker image...
                - docker build -t ${ImagingECRRepository}:${ImageTag} .
                - docker tag ${ImagingECRRepository}:${ImageTag} ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ImagingECRRepository}:${ImageTag}
            post_build:
              commands:
                - echo Build completed on `date`
                - echo Pushing the Docker image...
                - docker push ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ImagingECRRepository}:${ImageTag}
      SourceVersion: main

  ImagingStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      RoleArn: !GetAtt StepFunctionsExecutionRole.Arn
      DefinitionString: !Sub
        - |
          {
            "StartAt": "iterate_over_subjects",
            "States": {
              "iterate_over_subjects": {
                "ItemsPath": "$.Subject",
                "MaxConcurrency": 50,
                "Type": "Map",
                "Next": "Finish",
                "Iterator": {
                  "StartAt": "DICOM/NIfTI Conversion and Radiomic Feature Extraction",
                  "States": {
                    "Fallback": {
                      "Type": "Pass",
                      "Result": "This iteration failed for some reason",
                      "End": true
                    },
                    "DICOM/NIfTI Conversion and Radiomic Feature Extraction": {
                      "Type": "Task",
                      "OutputPath": "$.ProcessingJobArn",
                      "Resource": "arn:aws:states:::sagemaker:createProcessingJob.sync",
                      "Retry": [
                        {
                          "ErrorEquals": [
                            "SageMaker.AmazonSageMakerException"
                          ],
                          "IntervalSeconds": 15,
                          "MaxAttempts": 8,
                          "BackoffRate": 1.5
                        }
                      ],
                      "Catch": [
                        {
                          "ErrorEquals": [
                            "States.TaskFailed"
                          ],
                          "Next": "Fallback"
                        }
                      ],
                      "Parameters": {
                        "ProcessingJobName.$": "States.Format('{}-{}', $$.Execution.Input['PreprocessingJobName'], $)",
                        "ProcessingInputs": [
                          {
                            "InputName": "DICOM",
                            "AppManaged": false,
                            "S3Input": {
                              "S3Uri.$": "States.Format('s3://sagemaker-solutions-prod-${AWS::Region}/sagemaker-lung-cancer-survival-prediction/1.1.0/data/nsclc_radiogenomics/{}' , $)", 
                              "LocalPath": "/opt/ml/processing/input",
                              "S3DataType": "S3Prefix",
                              "S3InputMode": "File",
                              "S3DataDistributionType": "FullyReplicated",
                              "S3CompressionType": "None"
                            }
                          }
                        ],
                        "ProcessingOutputConfig": {
                          "Outputs": [
                            {
                              "OutputName": "CT-Nifti",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/CT-Nifti",
                                "LocalPath": "/opt/ml/processing/output/CT-Nifti",
                                "S3UploadMode": "EndOfJob"
                              }
                            },
                            {
                              "OutputName": "CT-SEG",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/CT-SEG",
                                "LocalPath": "/opt/ml/processing/output/CT-SEG",
                                "S3UploadMode": "EndOfJob"
                              }
                            },
                            {
                              "OutputName": "PNG",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/PNG",
                                "LocalPath": "/opt/ml/processing/output/PNG",
                                "S3UploadMode": "EndOfJob"
                              }
                            },
                            {
                              "OutputName": "CSV",
                              "AppManaged": false,
                              "S3Output": {
                                "S3Uri": "${S3Bucket}/nsclc_radiogenomics/CSV",
                                "LocalPath": "/opt/ml/processing/output/CSV",
                                "S3UploadMode": "EndOfJob"
                              }
                            }
                          ]
                        },
                        "AppSpecification": {
                          "ImageUri": "${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ImagingECRRepository}:${ImageTag}",
                          "ContainerArguments.$": "States.Array('--subject', $)",
                          "ContainerEntrypoint": [
                            "python3",
                            "/opt/dcm2nifti_processing.py"
                          ]
                        },
                        "RoleArn": "${SageMakerExecutionRoleArn}",
                        "ProcessingResources": {
                          "ClusterConfig": {
                            "InstanceCount": 1,
                            "InstanceType": "ml.m5.xlarge",
                            "VolumeSizeInGB": 5
                          }
                        }
                      },
                      "End": true
                    }
                  }
                }
              },
              "Finish": {
                "Type": "Succeed"
              }
            }
          }

        - {
            S3Bucket: !Sub "s3://${S3Bucket}",
            SageMakerExecutionRoleArn: !GetAtt SageMakerExecutionRole.Arn,
          }

  DataRetrievalRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource:
                  - !GetAtt S3Bucket.Arn
                  - !Sub "${S3Bucket.Arn}/*"
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - "arn:aws:s3:::aws-hcls-ml"
                  - "arn:aws:s3:::aws-hcls-ml/*"
        - PolicyName: CloudWatchLogs
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/*"
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/*:*"

  DockerPushRole:
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: Allow
              Principal:
                Service: codebuild.amazonaws.com
              Action: sts:AssumeRole
        Policies:
          - PolicyName: ECRAccess
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - ecr:GetAuthorizationToken
                  Resource: "*" # This action requires resource '*'
                - Effect: Allow
                  Action:
                    - ecr:ListImages
                    - ecr:BatchCheckLayerAvailability
                    - ecr:GetDownloadUrlForLayer
                    - ecr:BatchGetImage
                    - ecr:InitiateLayerUpload
                    - ecr:UploadLayerPart
                    - ecr:CompleteLayerUpload
                    - ecr:PutImage
                  Resource:
                    - !Sub "arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${ECRRepository}"
          - PolicyName: CloudWatchLogs
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - logs:CreateLogGroup
                    - logs:CreateLogStream
                    - logs:PutLogEvents
                  Resource:
                    - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/*"
                    - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/*:*"
          - PolicyName: GitCloneAccess
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - codecommit:GitPull
                  Resource: !Sub "arn:aws:codecommit:${AWS::Region}:${AWS::AccountId}:${GithubLink}"

  ImagingDockerBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: ECRAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - ecr:GetAuthorizationToken
                Resource: "*"
              - Effect: Allow
                Action:
                  - ecr:ListImages
                  - ecr:BatchCheckLayerAvailability
                  - ecr:GetDownloadUrlForLayer
                  - ecr:BatchGetImage
                  - ecr:InitiateLayerUpload
                  - ecr:UploadLayerPart
                  - ecr:CompleteLayerUpload
                  - ecr:PutImage
                Resource:
                  - !Sub "arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/medical-image-processing-smstudio"
        - PolicyName: S3Access
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  - !Sub "${S3Bucket.Arn}/Imaginglambdafunction.zip"
        - PolicyName: CloudWatchLogs
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/*"
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/*:*"

  StepFunctionsExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: StepFunctionsExecutionPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              # SageMaker permissions
              - Effect: Allow
                Action:
                  - "sagemaker:CreateProcessingJob"
                  - "sagemaker:DescribeProcessingJob"
                  - "sagemaker:StopProcessingJob"
                  - "sagemaker:ListTags"
                  - "sagemaker:AddTags"
                Resource: !Sub "arn:aws:sagemaker:${AWS::Region}:${AWS::AccountId}:processing-job/dcm-nifti-*"
              # IAM permissions
              - Effect: Allow
                Action:
                  - "iam:PassRole"
                Resource: !GetAtt SageMakerExecutionRole.Arn
              # Step Functions Map state permissions
              - Effect: Allow
                Action:
                  - "events:PutTargets"
                  - "events:PutRule"
                  - "events:DescribeRule"
                  - "events:DeleteRule"
                  - "events:DisableRule"
                  - "events:EnableRule"
                  - "events:RemoveTargets"
                Resource:
                  - !Sub "arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/StepFunctions-Map-*"
                  - !Sub "arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/StepFunctions-*"
              # States permissions for Map state
              - Effect: Allow
                Action:
                  - "states:StartExecution"
                  - "states:StopExecution"
                  - "states:DescribeExecution"
                  - "states:CreateStateMachine"
                  - "states:DeleteStateMachine"
                  - "states:UpdateStateMachine"
                  - "states:DescribeStateMachine"
                  - "states:DescribeStateMachineForExecution"
                  - "states:ListExecutions"
                Resource:
                  - !Sub "arn:aws:states:${AWS::Region}:${AWS::AccountId}:stateMachine:*"
                  - !Sub "arn:aws:states:${AWS::Region}:${AWS::AccountId}:execution:*:*"
              # CloudWatch Logs permissions
              - Effect: Allow
                Action:
                  - "logs:CreateLogDelivery"
                  - "logs:GetLogDelivery"
                  - "logs:UpdateLogDelivery"
                  - "logs:DeleteLogDelivery"
                  - "logs:ListLogDeliveries"
                  - "logs:PutLogEvents"
                  - "logs:PutResourcePolicy"
                  - "logs:DescribeResourcePolicies"
                  - "logs:DescribeLogGroups"
                Resource:
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - "lambda:InvokeFunction"
                Resource: !Sub "arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:*"
        - PolicyName: EventsPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "events:PutTargets"
                  - "events:PutRule"
                  - "events:DescribeRule"
                Resource:
                  - !Sub "arn:aws:events:${AWS::Region}:${AWS::AccountId}:rule/*"

  SageMakerExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: sagemaker.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: SageMakerProcessingAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              # Processing Job Permissions
              - Effect: Allow
                Action:
                  - "sagemaker:CreateProcessingJob"
                  - "sagemaker:DescribeProcessingJob"
                  - "sagemaker:StopProcessingJob"
                  - "sagemaker:ListProcessingJobs"
                Resource: !Sub "arn:aws:sagemaker:${AWS::Region}:${AWS::AccountId}:processing-job/*"
              # ECR Access for Container
              - Effect: Allow
                Action:
                  - "ecr:GetAuthorizationToken"
                Resource: "*"
              - Effect: Allow
                Action:
                  - "ecr:BatchCheckLayerAvailability"
                  - "ecr:GetDownloadUrlForLayer"
                  - "ecr:BatchGetImage"
                Resource:
                  - !Sub "arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${ImagingECRRepository}"
        - PolicyName: S3Access
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              # Input data access
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                  - "s3:ListBucket"
                Resource:
                  - "arn:aws:s3:::sagemaker-solutions-prod-*"
                  - "arn:aws:s3:::sagemaker-solutions-prod-*/sagemaker-lung-cancer-survival-prediction/*"
              # Output data access
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:ListBucket"
                Resource:
                  - !Sub "arn:aws:s3:::${S3Bucket}"
                  - !Sub "arn:aws:s3:::${S3Bucket}/nsclc_radiogenomics/*"
        - PolicyName: CloudWatchLogsAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                  - "logs:DescribeLogStreams"
                Resource:
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/sagemaker/ProcessingJobs:*"
                  - !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/sagemaker/ProcessingJobs:log-stream:*"
        - PolicyName: KMSAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "kms:Decrypt"
                  - "kms:GenerateDataKey"
                Resource: !Sub "arn:aws:kms:${AWS::Region}:${AWS::AccountId}:key/*"
                Condition:
                  StringEquals:
                    "kms:ViaService": !Sub "sagemaker.${AWS::Region}.amazonaws.com"
        - PolicyName: NetworkAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "ec2:CreateNetworkInterface"
                  - "ec2:CreateNetworkInterfacePermission"
                  - "ec2:DeleteNetworkInterface"
                  - "ec2:DeleteNetworkInterfacePermission"
                  - "ec2:DescribeNetworkInterfaces"
                  - "ec2:DescribeVpcs"
                  - "ec2:DescribeDhcpOptions"
                  - "ec2:DescribeSubnets"
                  - "ec2:DescribeSecurityGroups"
                Resource: "*"
                Condition:
                  StringEquals:
                    "aws:RequestedRegion": !Ref "AWS::Region"

  CodeBuildServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: CodeBuildAccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/${AWS::StackName}-AgentandUIBuild:*
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*
              - Effect: Allow
                Action:
                  - cloudformation:PackageTemplate
                Resource:
                  - !Sub arn:aws:s3:::${S3Bucket}
                  - !Sub arn:aws:s3:::${S3Bucket}/*

  # Bedrock Knowledge Base
  BedrockKBExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: !Sub 'AmazonBedrockExecutionRoleForKnowledgeBase_${EnvironmentName}_${AWS::Region}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: 
                - bedrock.amazonaws.com
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref 'AWS::AccountId'
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:knowledge-base/*'
      Policies:
      - PolicyName: FoundationModelPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - bedrock:InvokeModel
            Resource: 
              - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/amazon.titan-embed-text-v1'
              - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'
            Sid: BedrockInvokeModelStatement
      - PolicyName: OSSPolicy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - aoss:APIAccessAll
            Resource: !Sub 'arn:aws:aoss:${AWS::Region}:${AWS::AccountId}:collection/${Collection}'
            Sid: OpenSearchServerlessAPIAccessAllStatement
      - PolicyName: S3Policy
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:ListBucket
            Resource: !Sub 'arn:aws:s3:::${S3Bucket}'
            Sid: S3ListBucketStatement
          - Effect: Allow
            Action:
            - s3:GetObject
            Resource: !Sub 'arn:aws:s3:::${S3Bucket}/*'
            Sid: S3GetObjectStatement

  EncryptionPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: !Sub '${EnvironmentName}-encryption-policy'
      Type: encryption
      Description: !Sub 'Encryption policy for ${AWS::StackName} collection'
      Policy: !Sub |
        {
          "Rules": [
            {
              "ResourceType": "collection",
              "Resource": ["collection/${EnvironmentName}-${VectorStoreName}"]
            }
          ],
          "AWSOwnedKey": true
        }

  NetworkPolicy:
    Type: 'AWS::OpenSearchServerless::SecurityPolicy'
    Properties:
      Name: !Sub '${EnvironmentName}-network-policy'
      Type: network
      Description: !Sub 'Network policy for ${AWS::StackName} collection'
      Policy: !Sub |
        [
          {
            "Rules": [
              {
                "ResourceType": "collection",
                "Resource": ["collection/${EnvironmentName}-${VectorStoreName}"]
              }
            ],
            "AllowFromPublic": true
          }
        ]

  Collection:
    Type: 'AWS::OpenSearchServerless::Collection'
    Properties:
      Name: !Sub '${EnvironmentName}-${VectorStoreName}'
      Type: VECTORSEARCH
      Description: !Sub 'Collection to hold vector for ${AWS::StackName}'
    DependsOn: 
      - EncryptionPolicy
      - NetworkPolicy

  OpenSearchIndexLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaBasicExecution
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: 'arn:aws:logs:*:*:*'
        - PolicyName: allowAoss
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - aoss:APIAccessAll
              - aoss:List*
              - aoss:Get*
              - aoss:Create*
              - aoss:Update*
              - aoss:Delete*
              Resource: 'arn:aws:logs:*:*:*'

  OpenSearchIndexLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt OpenSearchIndexLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          import time
          from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth

          def create_index_with_retry(client, index_name, index_body, max_retries=5, base_delay=5):
              for attempt in range(max_retries):
                  try:
                      response = client.indices.create(index=index_name, body=json.dumps(index_body))
                      print(f"Index created: {response}")
                      return True
                  except Exception as e:
                      print(f"Attempt {attempt + 1} failed: {str(e)}")
                      if attempt < max_retries - 1:
                          delay = base_delay * (2 ** attempt)  # Exponential backoff
                          print(f"Retrying in {delay} seconds...")
                          time.sleep(delay)
                      else:
                          print("Max retries reached. Index creation failed.")
                          return False

          def handler(event, context):
              if event['RequestType'] in ['Create', 'Update']:
                  try:
                      collection_name = event['ResourceProperties']['CollectionName']
                      index_name = event['ResourceProperties']['IndexName']
                      collection_id = event["ResourceProperties"]["CollectionId"]
                      region = event["ResourceProperties"]["Region"]
                      
                      print(f"Collection Name: {collection_name}")
                      print(f"Index Name: {index_name}")
                      print(f"Collection ID: {collection_id}")
                      print(f"Region: {region}")
                      
                      service = 'aoss'
                      host = f"{collection_id}.{region}.{service}.amazonaws.com"
                      credentials = boto3.Session().get_credentials()
                      awsauth = AWSV4SignerAuth(credentials, region, service)
                      
                      client = OpenSearch(
                          hosts=[{'host': host, 'port': 443}],
                          http_auth=awsauth,
                          use_ssl=True,
                          verify_certs=True,
                          connection_class=RequestsHttpConnection,
                          pool_maxsize=20,
                      )
                      
                      # Updated index_body to match Amazon Titan model specifications
                      index_body = {
                          "settings": {
                              "index": {
                                  "knn": True,
                                  "knn.algo_param.ef_search": 512
                              }
                          },
                          "mappings": {
                              "properties": {
                                  "bedrock-knowledge-base-default-vector": {
                                      "type": "knn_vector",
                                      "dimension": 1536,
                                      "method": {
                                          "name": "hnsw",
                                          "engine": "faiss",
                                          "parameters": {
                                              "ef_construction": 512,
                                              "m": 16
                                          },
                                          "space_type": "l2",
                                      },
                                  },
                                  "AMAZON_BEDROCK_METADATA": {
                                      "type": "text",
                                      "index": "false"
                                  },
                                  "AMAZON_BEDROCK_TEXT_CHUNK": {
                                      "type": "text",
                                      "index": "true"
                                  },
                              }
                          }
                      }
                      
                      # Initial delay before attempting to create the index
                      print("Waiting 50 seconds before attempting to create the index...")
                      time.sleep(50)
                      
                      # Attempt to create the index with retry logic
                      if create_index_with_retry(client, index_name, index_body):
                          print("Waiting 60 seconds for the index to be fully created...")
                          time.sleep(60)
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      else:
                          raise Exception("Failed to create index after multiple attempts")
                  
                  except Exception as e:
                      print(f"Error: {str(e)}")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {})
              else:
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

      Runtime: python3.12
      Timeout: 900
      Layers: 
        - !FindInMap [RegionMap, !Ref 'AWS::Region', PandasLayer]

  CreateOpenSearchIndex:
    Type: Custom::CreateOpenSearchIndex
    Properties:
      ServiceToken: !GetAtt OpenSearchIndexLambda.Arn
      CollectionName: !Sub '${AWS::StackName}-${VectorStoreName}'
      IndexName: !Ref IndexName
      CollectionId: !GetAtt Collection.Id
      Region: !Ref 'AWS::Region'
    DependsOn:
      - Collection
      - BedrockOSSPolicyForKB

  BedrockOSSPolicyForKB:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub ${AWS::StackName}-bedrock-oss-policy
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: OpenSearchServerlessAPIAccessAllStatement
            Effect: Allow
            Action:
              - 'aoss:APIAccessAll'
            Resource: !GetAtt Collection.Arn
      Roles:
        - !Ref BedrockKBExecutionRole
        - !Ref OpenSearchIndexLambdaRole

  DataAccessPolicy:
    Type: 'AWS::OpenSearchServerless::AccessPolicy'
    Properties:
      Name: !Sub '${EnvironmentName}-access-policy'
            
      Type: data
      Description: !Sub 'Access policy for ${AWS::StackName} collection'
      Policy: !Sub |
        [
          {
            "Rules": [
              {
                "ResourceType": "collection",
                "Resource": ["collection/${EnvironmentName}-${VectorStoreName}"],
                "Permission": [
                  "aoss:CreateCollectionItems",
                  "aoss:DeleteCollectionItems",
                  "aoss:UpdateCollectionItems",
                  "aoss:DescribeCollectionItems"
                ]
              },
              {
                "ResourceType": "index",
                "Resource": ["index/${EnvironmentName}-${VectorStoreName}/*"],
                "Permission": [
                  "aoss:CreateIndex",
                  "aoss:DeleteIndex",
                  "aoss:UpdateIndex",
                  "aoss:DescribeIndex",
                  "aoss:ReadDocument",
                  "aoss:WriteDocument"
                ]
              }
            ],
            "Principal": [
              "${BedrockKBExecutionRole.Arn}",
              "arn:aws:iam::${AWS::AccountId}:role/aws-service-role/bedrock.amazonaws.com/AWSServiceRoleForAmazonBedrock",
              "${OpenSearchIndexLambdaRole.Arn}"
            ],
            "Description": "Data access policy for OpenSearch and Bedrock"
          }
        ]
    DependsOn:
      - Collection
      - BedrockKBExecutionRole
      - OpenSearchIndexLambdaRole

  BedrockKnowledgeBase:
    Type: AWS::Bedrock::KnowledgeBase
    Properties:
      Name: !Sub '${AWS::StackName}-ncbiKnowledgebase'
      Description: "Knowledgebase containing information for clinical research"
      KnowledgeBaseConfiguration:
        Type: VECTOR
        VectorKnowledgeBaseConfiguration:
          EmbeddingModelArn: !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/amazon.titan-embed-text-v1'
      RoleArn: !GetAtt BedrockKBExecutionRole.Arn
      StorageConfiguration:
        Type: OPENSEARCH_SERVERLESS
        OpensearchServerlessConfiguration:
          CollectionArn: !GetAtt Collection.Arn
          FieldMapping: 
              VectorField: "bedrock-knowledge-base-default-vector"
              TextField: "text"
              MetadataField: "metadata"
          VectorIndexName: !Ref IndexName
    DependsOn: 
      - DataAccessPolicy
      - CreateOpenSearchIndex

  KnowledgeBaseDataSource:
    Type: AWS::Bedrock::DataSource
    DependsOn:
      - BedrockKnowledgeBase
    Properties:
      DataSourceConfiguration:
        Type: S3
        S3Configuration:
          BucketArn: !Sub 'arn:aws:s3:::${S3Bucket}'
          InclusionPrefixes:
            - "ncbi_article.pdf"
      Description: !Sub 'Knowledge base Data Source for ${AWS::StackName} (NCBI article)'
      KnowledgeBaseId: !Ref BedrockKnowledgeBase
      Name: !Sub '${AWS::StackName}-ncbiKnowledgebaseds'
      VectorIngestionConfiguration:
        ChunkingConfiguration:
          ChunkingStrategy: FIXED_SIZE
          FixedSizeChunkingConfiguration:
            MaxTokens: 300
            OverlapPercentage: 20

  ScientificPlotLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
      Policies:
        - PolicyName: S3ObjectPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource: !Sub 'arn:aws:s3:::${S3Bucket}/*'
        - PolicyName: ECRAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ecr:GetAuthorizationToken
                Resource: '*'
              - Effect: Allow
                Action:
                  - ecr:BatchGetImage
                  - ecr:GetDownloadUrlForLayer
                  - ecr:BatchCheckLayerAvailability
                Resource:
                  - !Sub 'arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${ECRRepository}'

  ScientificPlotLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: ScientificPlotLambda
      PackageType: Image
      Code:
        ImageUri: !Sub ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ECRRepository}:${ImageTag}
      Role: !GetAtt ScientificPlotLambdaExecutionRole.Arn
      MemorySize: 512
      Timeout: 900
      Environment:
        Variables:
          S3_BUCKET: !Ref S3Bucket

Outputs:
  RedshiftClusterEndpoint:
    Description: Redshift Cluster Endpoint
    Value: !GetAtt RedshiftCluster.Endpoint.Address

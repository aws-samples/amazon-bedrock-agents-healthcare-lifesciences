{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245b51d8",
   "metadata": {},
   "source": [
    "## Evaluate Biomarker Supervisor Agent\n",
    "\n",
    "In this notebook we demonstrate how to use AgentCore Evaluations with our biomarker supervisor agent using on-demand built-in evaluators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3084a5",
   "metadata": {},
   "source": [
    "#### Upgrade AgentCore and OpenTelemetry dependencies to latest versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade bedrock-agentcore bedrock-agentcore-starter-toolkit aws-opentelemetry-distro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6920313",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need to complete notebook [05-multi_agent_biomarker_strands.ipynb](05-multi_agent_biomarker_strands.ipynb) and have the agent runtime deployed on Bedrock AgentCore. Also make sure you performed the Bedrock AgentCore Observability setup exaplined on [00-setup_environment.ipynb](00-setup_environment.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72861556",
   "metadata": {},
   "source": [
    "## On-demand Evaluations\n",
    "\n",
    "On-demand evaluation provides a flexible way to evaluate specific agent interactions by directly analyzing a chosen set of spans. Unlike online evaluation which continuously monitors production traffic, on-demand evaluation lets you perform targeted assessments of selected interactions at any time.\n",
    "\n",
    "With on-demand evaluation, you specify the exact spans, traces or sessions you want to evaluate by providing their span, trace or session IDs. When using the AgentCore Starter toolkit you can also automatically evaluate all traces in a session.\n",
    "\n",
    "You can then apply custom evaluators or built-in evaluators to your agent's interactions. This evaluation type is particularly useful when you need to investigate specific customer interactions, validate fixes for reported issues, or analyze historical data for quality improvements. Once you submit the evaluation request, the service processes only the specified spans and provides detailed results for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e54719",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Evaluation, Observability\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "from boto3.session import Session\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126a172",
   "metadata": {},
   "source": [
    "### Generating Data for Evaluations\n",
    "\n",
    "We are going to invoke the agent using sample questions within a session and will use the built in evaluators to see how our agent is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3 = Session()\n",
    "region = boto3.region_name\n",
    "\n",
    "ssm_client = boto3.client('ssm', region)\n",
    "agent_arn = ssm_client.get_parameter(Name='/streamlitapp/env1/AGENT_ARN')['Parameter']['Value']\n",
    "session_id = str(uuid.uuid4())\n",
    "print(f\"Session ID: {session_id}\")\n",
    "print(agent_arn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How many patients are current smokers?\",\n",
    "    \"What is the average age of patients diagnosed with Adenocarcinoma?\",\n",
    "    \"Can you search PubMed for evidence around the effects of biomarker use in oncology on clinical trial failure risk?\",\n",
    "    \"What are the FDA approved biomarkers for non small cell lung cancer?\",\n",
    "    \"According to literature evidence, what metagene cluster does gdf15 belong to\",\n",
    "    \"What properties of the tumor are associated with metagene 19 activity and EGFR pathway\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a6079",
   "metadata": {},
   "source": [
    "Iterate over list of questions invoking the agent hosted on Bedrock AgentCore. Please note that this next cell **will take around 5 minutes** to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "def invoke_agentcore(test_query : str):\n",
    "    response = agentcore_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        qualifier=\"DEFAULT\",\n",
    "        payload=json.dumps({\"prompt\": test_query}),\n",
    "        runtimeSessionId=session_id\n",
    "    )\n",
    "\n",
    "    print(f\"Testing orchestrator agent boto3 client: {test_query}\")\n",
    "    print(\"=\" * (41 + len(test_query)))\n",
    "\n",
    "    if \"text/event-stream\" in response.get(\"contentType\", \"\"):\n",
    "        # Processing streaming response\n",
    "        for line in response[\"response\"].iter_lines(chunk_size=1):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    # remove the SSE structure\n",
    "                    data = line[6:]\n",
    "                    # we need to parse it twice to convert from JSON str to a dictionary\n",
    "                    data_obj = json.loads(data)\n",
    "                    data_obj = json.loads(data_obj)\n",
    "                    # for this example we only care about the data field\n",
    "                    if \"data\" in data_obj:\n",
    "                        print(data_obj.get(\"data\"))\n",
    "    else:\n",
    "        # Handle non-streaming response\n",
    "        try:\n",
    "            events = []\n",
    "            for event in response.get(\"response\", []):\n",
    "                events.append(event)\n",
    "        except Exception as e:\n",
    "            events = [f\"Error reading EventStream: {e}\"]\n",
    "        if events:\n",
    "            try:\n",
    "                response_data = json.loads(events[0].decode(\"utf-8\"))\n",
    "                display(Markdown(response_data))\n",
    "            except:\n",
    "                print(f\"Raw response: {events[0]}\")\n",
    "\n",
    "for question in questions:\n",
    "    invoke_agentcore(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725ea7f",
   "metadata": {},
   "source": [
    "### Initialize AgentCore Evaluations Client\n",
    "\n",
    "Now let's initiate the AgentCore Evaluations client from the AgentCore Starter toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_client = Evaluation(region=region)\n",
    "\n",
    "# extract agent id from agent arn\n",
    "agent_id = agent_arn.rsplit('/', 1)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d4956",
   "metadata": {},
   "source": [
    "You can use the ```list_evaluators()``` function to see a list of build in evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_evaluators = eval_client.list_evaluators()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9792c",
   "metadata": {},
   "source": [
    "### Running Evaluations\n",
    "\n",
    "To run AgentCore Evaluations, you must provide session, trace or span information. Different metrics require different level of information from your agent traces, as we saw in the table with built-in evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7cd189",
   "metadata": {},
   "source": [
    "#### Trace level metrics\n",
    "\n",
    "- **Builtin.Coherence:** Evaluates whether the response is logically structured and coherent.\n",
    "- **Builtin.Conciseness:** Evaluates whether the response is appropriately brief without missing key information.\n",
    "- **Builtin.Correctness:** Evaluates whether the information in the agent's response is factually accurate.\n",
    "- **Builtin.InstructionFollowing:** Measures how well the agent follows the provided system instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ca65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_level_results = eval_client.run(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id, \n",
    "    evaluators=[\"Builtin.Coherence\", \"Builtin.Conciseness\", \"Builtin.Correctness\", \"Builtin.InstructionFollowing\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d64a5",
   "metadata": {},
   "source": [
    "#### Session level metrics\n",
    "\n",
    "- **Builtin.GoalSuccessRate:** Evaluates whether the conversation successfully meets the user's goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_sucess_results = eval_client.run(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id, \n",
    "    evaluators=[\"Builtin.GoalSuccessRate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d019f1",
   "metadata": {},
   "source": [
    "#### Tool level metrics\n",
    "\n",
    "- **Builtin.ToolSelectionAccuracy:** Component Level Metric. Evaluates whether the agent selected the appropriate tool for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_selection_results = eval_client.run(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id, \n",
    "    evaluators=[\"Builtin.ToolSelectionAccuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138c913",
   "metadata": {},
   "source": [
    "### Analyzing Evaluation Results\n",
    "\n",
    "Let's now analyze the results. In this case, we are evaluating the session with two different metrics in the same run. That means that we now need to know which evaluator is producing each response. We can do that with the evaluator_name property of the result. Let's see how well our agent used tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in trace_level_results.results:\n",
    "    if result.label != None:\n",
    "        information = f\"\"\"\n",
    "        {result.evaluator_name} Result: {result.label} ({result.value})\n",
    "        Explanation: \\n{result.explanation}]\\n\n",
    "        Token Usage: {result.token_usage}\\n\n",
    "        Context: {result.context}\\n\n",
    "        \"\"\"\n",
    "        print(\"===================================================\")\n",
    "        display(Markdown(information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in goal_sucess_results.results:\n",
    "    information = f\"\"\"\n",
    "    {result.evaluator_name} Result: {result.label} ({result.value})\n",
    "    Explanation: \\n{result.explanation}]\\n\n",
    "    Token Usage: {result.token_usage}\\n\n",
    "    Context: {result.context}\\n\n",
    "    \"\"\"\n",
    "    print(\"===================================================\")\n",
    "    display(Markdown(information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ff3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in tool_selection_results.results:\n",
    "    if result.label != None:\n",
    "        information = f\"\"\"\n",
    "        {result.evaluator_name} Result: {result.label} ({result.value})\n",
    "        Explanation: \\n{result.explanation}]\\n\n",
    "        Token Usage: {result.token_usage}\\n\n",
    "        Context: {result.context}\\n\n",
    "        \"\"\"\n",
    "        print(\"===================================================\")\n",
    "        display(Markdown(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba731db8",
   "metadata": {},
   "source": [
    "### Saving evaluation results\n",
    "\n",
    "The AgentCore starter toolkit also helps you saving the results of your agent evaluation in structured output files. To do so all you need to provide is the ouput parameter during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_sucess_results = eval_client.run(\n",
    "    agent_id=agent_id,\n",
    "    session_id=session_id, \n",
    "    evaluators=[\"Builtin.GoalSuccessRate\"],\n",
    "    output=f\"eval_results/{session_id}_goal_sucess.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16ac4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcls-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
